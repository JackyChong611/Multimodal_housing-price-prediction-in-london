{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03643b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏙️ Scraping London Core\n",
      "Current total: 0 properties\n",
      "Page 1: 25 properties\n",
      "Page 2: 25 properties\n",
      "Page 3: 25 properties\n",
      "Page 4: 25 properties\n",
      "Page 5: 25 properties\n",
      "Page 6: 25 properties\n",
      "Page 7: 25 properties\n",
      "Page 8: 25 properties\n",
      "Page 9: 25 properties\n",
      "Page 10: 25 properties\n",
      "Page 11: 25 properties\n",
      "Page 12: 25 properties\n",
      "Page 13: 25 properties\n",
      "Page 14: 25 properties\n",
      "Page 15: 25 properties\n",
      "Page 16: 25 properties\n",
      "Page 17: 25 properties\n",
      "Page 18: 25 properties\n",
      "Page 19: 25 properties\n",
      "Page 20: 25 properties\n",
      "Page 21: 25 properties\n",
      "Page 22: 25 properties\n",
      "Page 23: 25 properties\n",
      "Page 24: 25 properties\n",
      "Page 25: 25 properties\n",
      "Page 26: 25 properties\n",
      "Page 27: 25 properties\n",
      "Page 28: 25 properties\n",
      "Page 29: 25 properties\n",
      "Page 30: 25 properties\n",
      "Page 31: 25 properties\n",
      "Page 32: 25 properties\n",
      "Page 33: 25 properties\n",
      "Page 34: 25 properties\n",
      "Page 35: 25 properties\n",
      "Page 36: 25 properties\n",
      "Page 37: 25 properties\n",
      "Page 38: 25 properties\n",
      "Page 39: 25 properties\n",
      "Page 40: 25 properties\n",
      "Page 41: 25 properties\n",
      "Page 42: 25 properties\n",
      "Page 43: 0 properties\n",
      "Page 44: 0 properties\n",
      "Page 45: 0 properties\n",
      "Page 46: 0 properties\n",
      "Page 47: 0 properties\n",
      "Page 48: 0 properties\n",
      "Page 49: 0 properties\n",
      "Page 50: 0 properties\n",
      "Page 51: 0 properties\n",
      "Page 52: 0 properties\n",
      "Page 53: 0 properties\n",
      "Page 54: 0 properties\n",
      "Page 55: 0 properties\n",
      "Page 56: 0 properties\n",
      "Page 57: 0 properties\n",
      "Page 58: 0 properties\n",
      "Page 59: 0 properties\n",
      "Page 60: 0 properties\n",
      "Page 61: 0 properties\n",
      "Page 62: 0 properties\n",
      "Page 63: 0 properties\n",
      "Page 64: 0 properties\n",
      "Page 65: 0 properties\n",
      "Page 66: 0 properties\n",
      "Page 67: 0 properties\n",
      "Page 68: 0 properties\n",
      "Page 69: 0 properties\n",
      "Page 70: 0 properties\n",
      "Page 71: 0 properties\n",
      "Page 72: 0 properties\n",
      "Page 73: 0 properties\n",
      "Page 74: 0 properties\n",
      "Page 75: 0 properties\n",
      "Page 76: 0 properties\n",
      "Page 77: 0 properties\n",
      "Page 78: 0 properties\n",
      "Page 79: 0 properties\n",
      "Page 80: 0 properties\n",
      "Page 81: 0 properties\n",
      "Page 82: 0 properties\n",
      "Page 83: 0 properties\n",
      "Page 84: 0 properties\n",
      "Page 85: 0 properties\n",
      "Page 86: 0 properties\n",
      "Page 87: 0 properties\n",
      "Page 88: 0 properties\n",
      "Page 89: 0 properties\n",
      "Page 90: 0 properties\n",
      "Page 91: 0 properties\n",
      "Page 92: 0 properties\n",
      "Page 93: 0 properties\n",
      "Page 94: 0 properties\n",
      "Page 95: 0 properties\n",
      "Page 96: 0 properties\n",
      "Page 97: 0 properties\n",
      "Page 98: 0 properties\n",
      "Page 99: 0 properties\n",
      "Page 100: 0 properties\n",
      "Page 101: 0 properties\n",
      "Page 102: 0 properties\n",
      "Page 103: 0 properties\n",
      "Page 104: 0 properties\n",
      "Page 105: 0 properties\n",
      "Page 106: 0 properties\n",
      "Page 107: 0 properties\n",
      "Page 108: 0 properties\n",
      "Page 109: 0 properties\n",
      "Page 110: 0 properties\n",
      "Page 111: 0 properties\n",
      "Page 112: 0 properties\n",
      "Page 113: 0 properties\n",
      "Page 114: 0 properties\n",
      "Page 115: 0 properties\n",
      "Page 116: 0 properties\n",
      "Page 117: 0 properties\n",
      "Page 118: 0 properties\n",
      "Page 119: 0 properties\n",
      "Page 120: 0 properties\n",
      "Page 121: 0 properties\n",
      "Page 122: 0 properties\n",
      "Page 123: 0 properties\n",
      "Page 124: 0 properties\n",
      "Page 125: 0 properties\n",
      "Page 126: 0 properties\n",
      "Page 127: 0 properties\n",
      "Page 128: 0 properties\n",
      "Page 129: 0 properties\n",
      "Page 130: 0 properties\n",
      "Page 131: 0 properties\n",
      "Page 132: 0 properties\n",
      "Page 133: 0 properties\n",
      "Page 134: 0 properties\n",
      "Page 135: 0 properties\n",
      "Page 136: 0 properties\n",
      "Page 137: 0 properties\n",
      "Page 138: 0 properties\n",
      "Page 139: 0 properties\n",
      "Page 140: 0 properties\n",
      "Page 141: 0 properties\n",
      "Page 142: 0 properties\n",
      "Page 143: 0 properties\n",
      "Page 144: 0 properties\n",
      "Page 145: 0 properties\n",
      "Page 146: 0 properties\n",
      "Page 147: 0 properties\n",
      "Page 148: 0 properties\n",
      "Page 149: 0 properties\n",
      "Page 150: 0 properties\n",
      "Page 151: 0 properties\n",
      "Page 152: 0 properties\n",
      "Page 153: 0 properties\n",
      "Page 154: 0 properties\n",
      "Page 155: 0 properties\n",
      "Page 156: 0 properties\n",
      "Page 157: 0 properties\n",
      "Page 158: 0 properties\n",
      "Page 159: 0 properties\n",
      "Page 160: 0 properties\n",
      "Page 161: 0 properties\n",
      "Page 162: 0 properties\n",
      "Page 163: 0 properties\n",
      "Page 164: 0 properties\n",
      "Page 165: 0 properties\n",
      "Page 166: 0 properties\n",
      "Page 167: 0 properties\n",
      "Page 168: 0 properties\n",
      "Page 169: 0 properties\n",
      "Page 170: 0 properties\n",
      "Page 171: 0 properties\n",
      "Page 172: 0 properties\n",
      "Page 173: 0 properties\n",
      "Page 174: 0 properties\n",
      "Page 175: 0 properties\n",
      "Page 176: 0 properties\n",
      "Page 177: 0 properties\n",
      "Page 178: 0 properties\n",
      "Page 179: 0 properties\n",
      "Page 180: 0 properties\n",
      "Page 181: 0 properties\n",
      "Page 182: 0 properties\n",
      "Page 183: 0 properties\n",
      "Page 184: 0 properties\n",
      "Page 185: 0 properties\n",
      "Page 186: 0 properties\n",
      "Page 187: 0 properties\n",
      "Page 188: 0 properties\n",
      "Page 189: 0 properties\n",
      "Page 190: 0 properties\n",
      "Page 191: 0 properties\n",
      "Page 192: 0 properties\n",
      "Page 193: 0 properties\n",
      "Page 194: 0 properties\n",
      "Page 195: 0 properties\n",
      "Page 196: 0 properties\n",
      "Page 197: 0 properties\n",
      "Page 198: 0 properties\n",
      "Page 199: 0 properties\n",
      "Page 200: 0 properties\n",
      "Page 201: 0 properties\n",
      "Page 202: 0 properties\n",
      "Page 203: 0 properties\n",
      "Page 204: 0 properties\n",
      "Page 205: 0 properties\n",
      "Page 206: 0 properties\n",
      "Page 207: 0 properties\n",
      "Page 208: 0 properties\n",
      "Scraping 1050 properties...\n",
      "Completed 5/1050 properties\n",
      "Completed 10/1050 properties\n",
      "Completed 15/1050 properties\n",
      "Completed 20/1050 properties\n",
      "Completed 25/1050 properties\n",
      "Completed 30/1050 properties\n",
      "Completed 35/1050 properties\n",
      "Completed 40/1050 properties\n",
      "Completed 45/1050 properties\n",
      "Completed 50/1050 properties\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 392\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    391\u001b[0m     nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m--> 392\u001b[0m     results \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(run_complete_scraper())\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\windows_events.py:445\u001b[0m, in \u001b[0;36mIocpProactor.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results:\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n\u001b[0;32m    446\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\windows_events.py:774\u001b[0m, in \u001b[0;36mIocpProactor._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout too big\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     status \u001b[38;5;241m=\u001b[39m _overlapped\u001b[38;5;241m.\u001b[39mGetQueuedCompletionStatus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iocp, ms)\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    776\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import jmespath\n",
    "import asyncio\n",
    "from httpx import AsyncClient\n",
    "from parsel import Selector\n",
    "import nest_asyncio\n",
    "\n",
    "class RightmoveRentalScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.properties = []\n",
    "        self.httpx_client = AsyncClient(\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            },\n",
    "            follow_redirects=True,\n",
    "            http2=False,\n",
    "            timeout=15,\n",
    "        )\n",
    "    \n",
    "    def find_json_objects(self, text: str):\n",
    "        pos = 0\n",
    "        while True:\n",
    "            match = text.find(\"{\", pos)\n",
    "            if match == -1: break\n",
    "            try:\n",
    "                result, index = json.JSONDecoder().raw_decode(text[match:])\n",
    "                yield result\n",
    "                pos = match + index\n",
    "            except:\n",
    "                pos = match + 1\n",
    "\n",
    "    def extract_property_id(self, url):\n",
    "        \"\"\"Extract property ID from URL\"\"\"\n",
    "        match = re.search(r'/properties/(\\d+)', url)\n",
    "        return match.group(1) if match else 'N/A'\n",
    "    \n",
    "    def format_address_and_postcode(self, address_data):\n",
    "        \"\"\"Extract and format address and postcode from JSON address object\"\"\"\n",
    "        if isinstance(address_data, dict):\n",
    "            # Extract display address and clean it\n",
    "            display_address = address_data.get('displayAddress', '')\n",
    "            if display_address:\n",
    "                # Remove line breaks and extra spaces\n",
    "                clean_address = re.sub(r'\\r\\n|\\r|\\n', ', ', display_address)\n",
    "                clean_address = re.sub(r',\\s*,', ',', clean_address)  # Remove double commas\n",
    "                clean_address = re.sub(r'\\s+', ' ', clean_address).strip()  # Normalize spaces\n",
    "                \n",
    "                # Remove postcode from address if it appears at the end\n",
    "                postcode_pattern = r',?\\s*[A-Z]{1,2}\\d{1,2}[A-Z]?\\s*\\d[A-Z]{2}\\s*$'\n",
    "                clean_address = re.sub(postcode_pattern, '', clean_address, flags=re.IGNORECASE)\n",
    "                clean_address = clean_address.rstrip(', ')\n",
    "            else:\n",
    "                clean_address = 'N/A'\n",
    "            \n",
    "            # Combine outcode and incode to form postcode\n",
    "            outcode = address_data.get('outcode', '')\n",
    "            incode = address_data.get('incode', '')\n",
    "            \n",
    "            if outcode and incode:\n",
    "                postcode = f\"{outcode} {incode}\"\n",
    "            else:\n",
    "                postcode = 'N/A'\n",
    "            \n",
    "            return clean_address, postcode\n",
    "    \n",
    "    def extract_tenure_details(self, json_data):\n",
    "        \"\"\"Extract tenure type and lease years remaining from JSON data\"\"\"\n",
    "        tenure_type = 'N/A'\n",
    "        lease_years_remaining = 'N/A'\n",
    "        \n",
    "        if json_data:\n",
    "            # Extract tenure information\n",
    "            tenure_info = jmespath.search(\"tenure\", json_data)\n",
    "            \n",
    "            if isinstance(tenure_info, dict):\n",
    "                # Extract tenure type\n",
    "                tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                \n",
    "                # Extract years remaining on lease\n",
    "                years_remaining = tenure_info.get('yearsRemainingOnLease')\n",
    "                if years_remaining is not None:\n",
    "                    lease_years_remaining = years_remaining\n",
    "                \n",
    "                # Handle message field if needed\n",
    "                message = tenure_info.get('message')\n",
    "                if message:\n",
    "                    # You can append message to tenure_type if it contains useful info\n",
    "                    tenure_type = f\"{tenure_type} - {message}\"\n",
    "            \n",
    "            elif isinstance(tenure_info, str):\n",
    "                # If it's just a string, use it as tenure type\n",
    "                tenure_type = tenure_info\n",
    "        \n",
    "        return tenure_type, lease_years_remaining\n",
    "\n",
    "    async def extract_property_json_data(self, property_url):\n",
    "        try:\n",
    "            response = await self.httpx_client.get(property_url)\n",
    "            data = Selector(response.text).xpath(\"//script[contains(.,'PAGE_MODEL = ')]/text()\").get()\n",
    "            return list(self.find_json_objects(data))[0][\"propertyData\"]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    \n",
    "\n",
    "    def get_property_links(self, search_url, max_pages=1):\n",
    "        property_links = []\n",
    "        for page in range(max_pages):\n",
    "            page_url = f\"{search_url}&index={page * 24}\"\n",
    "            try:\n",
    "                response = self.session.get(page_url, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                links = [f\"https://www.rightmove.co.uk{link.get('href')}\" for link in soup.find_all('a', class_='propertyCard-link') if link.get('href')]\n",
    "                property_links.extend(links)\n",
    "                print(f\"Page {page + 1}: {len(links)} properties\")\n",
    "                time.sleep(0.5)\n",
    "            except:\n",
    "                break\n",
    "        return property_links\n",
    "\n",
    "    async def scrape_property_details_fast(self, property_url):\n",
    "        try:\n",
    "            html_task = asyncio.create_task(self.httpx_client.get(property_url))\n",
    "            json_data = await self.extract_property_json_data(property_url)\n",
    "            response = await html_task\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            property_data = {'url': property_url}\n",
    "            \n",
    "            # Extract property ID and use as title\n",
    "            property_id = self.extract_property_id(property_url)\n",
    "            property_data['title'] = property_id\n",
    "            \n",
    "            # Basic property info\n",
    "            bed_match = re.search(r'(\\d+)\\s*bed', soup.get_text(), re.I)\n",
    "            bath_match = re.search(r'(\\d+)\\s*bath', soup.get_text(), re.I)\n",
    "            property_data['bedrooms'] = bed_match.group(1) if bed_match else 'N/A'\n",
    "            property_data['bathrooms'] = bath_match.group(1) if bath_match else 'N/A'\n",
    "            \n",
    "\n",
    "            # Key features extraction\n",
    "            property_data['key_features'] = 'N/A'\n",
    "            feature_headers = soup.find_all(['h2', 'h3'], string=re.compile(r'key features|features|amenities', re.I))\n",
    "            for header in feature_headers:\n",
    "                next_element = header.find_next_sibling()\n",
    "                while next_element:\n",
    "                    if next_element.name in ['ul', 'ol']:\n",
    "                        features = [li.get_text().strip() for li in next_element.find_all('li')]\n",
    "                        property_data['key_features'] = '; '.join(features) if features else property_data['key_features']\n",
    "                        break\n",
    "                    next_element = next_element.find_next_sibling()\n",
    "                if property_data['key_features'] != 'N/A':\n",
    "                    break\n",
    "            \n",
    "            # Property details extraction\n",
    "            property_data.update({\n",
    "                'parking': 'N/A',\n",
    "                'garden': 'N/A',\n",
    "                'council_tax': 'N/A',\n",
    "                'accessibility': 'N/A',\n",
    "                'size_sqft': 'N/A',\n",
    "                'size_sqm': 'N/A',\n",
    "                'furish_status': 'N/A',\n",
    "            })\n",
    "           \n",
    "            # Extract property details from sections\n",
    "            detail_sections = soup.find_all('dt', class_='_17A0LehXZKxGHbPeiLQ1BI')\n",
    "            for section in detail_sections:\n",
    "                section_text = section.get_text().strip().upper()\n",
    "                value_element = section.find_next_sibling('dd')\n",
    "                value_text = value_element.get_text().strip() if value_element else 'N/A'\n",
    "                \n",
    "                property_data['parking'] = value_text if 'PARKING' in section_text else property_data['parking']\n",
    "                property_data['garden'] = value_text if 'GARDEN' in section_text else property_data['garden']\n",
    "                property_data['council_tax'] = value_text if 'COUNCIL TAX' in section_text else property_data['council_tax']\n",
    "                property_data['accessibility'] = value_text if 'ACCESSIBILITY' in section_text else property_data['accessibility']\n",
    "\n",
    "            furnish_dt = soup.find('dt', string=re.compile(r'^Furnish type:\\s*$', re.I))\n",
    "            if furnish_dt:\n",
    "                furnish_dd = furnish_dt.find_next_sibling('dd')\n",
    "                if furnish_dd:\n",
    "                    property_data['furish_status'] = furnish_dd.get_text().strip()\n",
    "\n",
    "            # Size extraction from full text\n",
    "            all_text = soup.get_text()\n",
    "            sqft_match = re.search(r'([\\d,]+)\\s*sq\\s*ft', all_text, re.I)\n",
    "            sqm_match = re.search(r'([\\d,]+)\\s*sq\\s*m', all_text, re.I)\n",
    "            property_data['size_sqft'] = sqft_match.group(1).replace(',', '') if sqft_match else 'N/A'\n",
    "            property_data['size_sqm'] = sqm_match.group(1).replace(',', '') if sqm_match else 'N/A'\n",
    "\n",
    "            # Stations and photos from JSON\n",
    "            if json_data:\n",
    "                stations = jmespath.search(\"nearestStations[*].{name: name, distance: distance}\", json_data) or []\n",
    "                photos = jmespath.search(\"images[*].{url: url, caption: caption}\", json_data) or []\n",
    "                floorplan = jmespath.search(\"floorplans[*].{url: url, caption: caption}\", json_data) or []\n",
    "                property_type = jmespath.search(\"propertySubType\", json_data) or jmespath.search(\"propertyType\", json_data) or 'N/A'\n",
    "                description = jmespath.search(\"text.description\", json_data) or 'N/A'\n",
    "                address_data = jmespath.search(\"address\", json_data) or 'N/A'\n",
    "                tenure_info = jmespath.search(\"tenure\", json_data) or {}\n",
    "                tenure_type = tenure_info.get('tenureType', 'N/A') if isinstance(tenure_info, dict) else (tenure_info if tenure_info != 'N/A' else 'N/A')\n",
    "                lease_years_remaining = tenure_info.get('yearsRemainingOnLease', 'N/A') if isinstance(tenure_info, dict) else 'N/A'\n",
    "                latitude = jmespath.search(\"location.latitude\", json_data) or 'N/A'\n",
    "                longitude = jmespath.search(\"location.longitude\", json_data) or 'N/A'\n",
    "                price = jmespath.search(\"prices.primaryPrice\", json_data) or 'N/A'\n",
    "                \n",
    "                formatted_address, postcode = self.format_address_and_postcode(address_data)\n",
    "                property_data.update({\n",
    "                    'nearest_stations': '; '.join([s['name'] for s in stations]) or 'N/A',\n",
    "                    'station_distances': '; '.join([f\"{s['distance']} miles\" for s in stations]) or 'N/A',\n",
    "                    'station_count': len(stations),\n",
    "                    'image_count': len(photos),\n",
    "                    'image_urls': '; '.join([p['url'] for p in photos]) or 'N/A',\n",
    "                    'floorplan_count': len(floorplan),\n",
    "                    'floorplan_urls': '; '.join([f['url'] for f in floorplan]) or 'N/A',\n",
    "                    'property_type': property_type,\n",
    "                    'tenure_type': tenure_type,\n",
    "                    'lease_years_remaining': lease_years_remaining,\n",
    "                    'description': description,\n",
    "                    'latitude': latitude,\n",
    "                    'longitude': longitude,\n",
    "                    'address': formatted_address,\n",
    "                    'postcode': postcode,\n",
    "                    'price': price\n",
    "                })\n",
    "            else:\n",
    "                # HTML fallback for property type if JSON fails\n",
    "                property_type_fallback = 'N/A'\n",
    "                type_element = soup.find(string=re.compile(r'(flat|house|apartment|studio|maisonette)', re.I))\n",
    "                if type_element:\n",
    "                    property_type_fallback = type_element.strip()\n",
    "                \n",
    "                property_data.update({\n",
    "                    'nearest_stations': 'N/A',\n",
    "                    'station_distances': 'N/A', \n",
    "                    'station_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'image_urls': 'N/A',\n",
    "                    'floorplan_count': 0,\n",
    "                    'floorplan_urls': 'N/A',\n",
    "                    'property_type': property_type_fallback,\n",
    "                    'description': 'N/A',\n",
    "                    'tenure_type': 'N/A',\n",
    "                    'lease_years_remaining': 'N/A',\n",
    "                    'latitude': 'N/A',\n",
    "                    'longitude': 'N/A',\n",
    "                    'address': 'N/A',\n",
    "                    'postcode': 'N/A',\n",
    "                    'price': 'N/A'\n",
    "                })\n",
    "            \n",
    "            return property_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'url': property_url, 'error': str(e)}\n",
    "\n",
    "    async def scrape_all_properties_fast(self, search_url, max_pages=2, max_properties=None):\n",
    "        property_links = self.get_property_links(search_url, max_pages)\n",
    "        property_links = property_links[:max_properties] if max_properties else property_links\n",
    "        \n",
    "        print(f\"Scraping {len(property_links)} properties...\")\n",
    "        \n",
    "        batch_size = 5\n",
    "        for i in range(0, len(property_links), batch_size):\n",
    "            batch = property_links[i:i+batch_size]\n",
    "            tasks = [self.scrape_property_details_fast(url) for url in batch]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            for result in results:\n",
    "                self.properties.append(result if isinstance(result, dict) else {'error': str(result)})\n",
    "            \n",
    "            print(f\"Completed {min(i+batch_size, len(property_links))}/{len(property_links)} properties\")\n",
    "            await asyncio.sleep(0.5)\n",
    "\n",
    "    def save_to_csv(self, filename='rightmove_housing price_test.csv'):\n",
    "        df = pd.DataFrame(self.properties)\n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].astype(str).apply(\n",
    "                lambda x: re.sub(r'<[^>]+>', ' ', x) if x != 'N/A' else x\n",
    "            ).apply(\n",
    "                lambda x: re.sub(r'\\s+', ' ', x).strip() if x != 'N/A' else x\n",
    "            )\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(self.properties)} properties to {filename}\")\n",
    "        return df\n",
    "\n",
    "async def run_complete_scraper():\n",
    "    scraper = RightmoveRentalScraper()\n",
    "    \n",
    "    # Multiple London area searches to get 50,000 properties\n",
    "    london_search_urls = [\n",
    "        # Greater London (main search)\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=0.0&_includeSSTC=on\",\n",
    "        \n",
    "        # Central London\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=Central+London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=0.0&_includeSSTC=on\",\n",
    "        \n",
    "        # North London\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=North+London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=0.0&_includeSSTC=on\",\n",
    "        \n",
    "        # South London\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=South+London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=0.0&_includeSSTC=on\",\n",
    "        \n",
    "        # East London\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=East+London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=0.0&_includeSSTC=on\",\n",
    "        \n",
    "        # West London\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=West+London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=0.0&_includeSSTC=on\",\n",
    "        \n",
    "        # Include surrounding areas within London commuter belt\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=Greater+London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=5.0&_includeSSTC=on\",\n",
    "        \n",
    "        # Extend radius to 10 miles from London center\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=10.0&_includeSSTC=on\",\n",
    "        \n",
    "        # Extend radius to 15 miles from London center\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=15.0&_includeSSTC=on\",\n",
    "        \n",
    "        # Include all property types and price ranges\n",
    "        \"https://www.rightmove.co.uk/property-for-sale/find.html?searchLocation=London&useLocationIdentifier=true&locationIdentifier=REGION%5E87490&buy=For+sale&radius=20.0&_includeSSTC=on&includeSSTC=true\"\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    target_properties = 50000\n",
    "    \n",
    "    for i, search_url in enumerate(london_search_urls):\n",
    "        # Extract search type from URL for logging\n",
    "        if \"radius=0.0\" in search_url:\n",
    "            search_type = \"London Core\"\n",
    "        elif \"radius=5.0\" in search_url:\n",
    "            search_type = \"London +5 miles\"\n",
    "        elif \"radius=10.0\" in search_url:\n",
    "            search_type = \"London +10 miles\"\n",
    "        elif \"radius=15.0\" in search_url:\n",
    "            search_type = \"London +15 miles\"\n",
    "        elif \"radius=20.0\" in search_url:\n",
    "            search_type = \"London +20 miles\"\n",
    "        else:\n",
    "            search_type = f\"London Area {i+1}\"\n",
    "        \n",
    "        print(f\"\\n🏙️ Scraping {search_type}\")\n",
    "        print(f\"Current total: {len(scraper.properties)} properties\")\n",
    "        \n",
    "        # Calculate how many more properties we need\n",
    "        remaining_needed = target_properties - len(scraper.properties)\n",
    "        if remaining_needed <= 0:\n",
    "            print(f\"✅ Target of {target_properties} properties reached!\")\n",
    "            break\n",
    "        \n",
    "        # Set max_properties for this search\n",
    "        max_for_this_search = min(remaining_needed, 10000)  # Don't take more than 10k per search\n",
    "        \n",
    "        await scraper.scrape_all_properties_fast(\n",
    "            search_url, \n",
    "            max_pages=500,  # Increased pages per search\n",
    "            max_properties=max_for_this_search\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ {search_type} completed. Total properties: {len(scraper.properties)}\")\n",
    "        \n",
    "        # Break if we've reached our target\n",
    "        if len(scraper.properties) >= target_properties:\n",
    "            print(f\"🎯 Target reached! Total: {len(scraper.properties)} properties\")\n",
    "            break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Trim to exactly 50,000 if we got more\n",
    "    if len(scraper.properties) > target_properties:\n",
    "        scraper.properties = scraper.properties[:target_properties]\n",
    "        print(f\"✂️ Trimmed to exactly {target_properties} properties\")\n",
    "    \n",
    "    scraper.save_to_csv('rightmove_london_50k_properties.csv')\n",
    "    await scraper.httpx_client.aclose()\n",
    "    \n",
    "    print(f\"⚡ Completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"📊 Final count: {len(scraper.properties)} London properties\")\n",
    "    return scraper.properties\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    results = asyncio.run(run_complete_scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85739d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 37 London search combinations\n",
      "\n",
      "🏙️ Scraping City of London (sale) - all_prices\n",
      "Current total: 0 properties\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Found 63 new properties to scrape\n",
      "Completed 5/63 properties from City of London\n",
      "Completed 10/63 properties from City of London\n",
      "Completed 15/63 properties from City of London\n",
      "Completed 20/63 properties from City of London\n",
      "Completed 25/63 properties from City of London\n",
      "Completed 30/63 properties from City of London\n",
      "Completed 35/63 properties from City of London\n",
      "Completed 40/63 properties from City of London\n",
      "Completed 45/63 properties from City of London\n",
      "Completed 50/63 properties from City of London\n",
      "Completed 55/63 properties from City of London\n",
      "Completed 60/63 properties from City of London\n",
      "Completed 63/63 properties from City of London\n",
      "✅ City of London (all_prices) completed. Total properties: 63\n",
      "\n",
      "🏙️ Scraping City of London (sale) - maxPrice=230000\n",
      "Current total: 63 properties\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Found 23 new properties to scrape\n",
      "Completed 5/23 properties from City of London\n",
      "Completed 10/23 properties from City of London\n",
      "Completed 15/23 properties from City of London\n",
      "Completed 20/23 properties from City of London\n",
      "Completed 23/23 properties from City of London\n",
      "✅ City of London (maxPrice=230000) completed. Total properties: 86\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=230000maxPrice=240000\n",
      "Current total: 86 properties\n",
      "Page 1: 2 properties found\n",
      "Page 2: 2 properties found\n",
      "Page 3: 2 properties found\n",
      "Page 4: 2 properties found\n",
      "Page 5: 2 properties found\n",
      "Page 6: 2 properties found\n",
      "Page 7: 2 properties found\n",
      "Page 8: 2 properties found\n",
      "Page 9: 2 properties found\n",
      "Page 10: 2 properties found\n",
      "Page 11: 2 properties found\n",
      "Page 12: 2 properties found\n",
      "Page 13: 2 properties found\n",
      "Page 14: 2 properties found\n",
      "Page 15: 2 properties found\n",
      "Page 16: 2 properties found\n",
      "Page 17: 2 properties found\n",
      "Page 18: 2 properties found\n",
      "Page 19: 2 properties found\n",
      "Page 20: 2 properties found\n",
      "Page 21: 2 properties found\n",
      "Page 22: 2 properties found\n",
      "Page 23: 2 properties found\n",
      "Page 24: 2 properties found\n",
      "Page 25: 2 properties found\n",
      "Page 26: 2 properties found\n",
      "Page 27: 2 properties found\n",
      "Page 28: 2 properties found\n",
      "Page 29: 2 properties found\n",
      "Page 30: 2 properties found\n",
      "Page 31: 2 properties found\n",
      "Page 32: 2 properties found\n",
      "Page 33: 2 properties found\n",
      "Page 34: 2 properties found\n",
      "Page 35: 2 properties found\n",
      "Page 36: 2 properties found\n",
      "Page 37: 2 properties found\n",
      "Page 38: 2 properties found\n",
      "Page 39: 2 properties found\n",
      "Page 40: 2 properties found\n",
      "Page 41: 2 properties found\n",
      "Page 42: 2 properties found\n",
      "No new properties found in City of London - minPrice=230000maxPrice=240000\n",
      "✅ City of London (minPrice=230000maxPrice=240000) completed. Total properties: 86\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=240000maxPrice=250000\n",
      "Current total: 86 properties\n",
      "Page 1: 9 properties found\n",
      "Page 2: 9 properties found\n",
      "Page 3: 9 properties found\n",
      "Page 4: 9 properties found\n",
      "Page 5: 9 properties found\n",
      "Page 6: 9 properties found\n",
      "Page 7: 9 properties found\n",
      "Page 8: 9 properties found\n",
      "Page 9: 9 properties found\n",
      "Page 10: 9 properties found\n",
      "Page 11: 9 properties found\n",
      "Page 12: 9 properties found\n",
      "Page 13: 9 properties found\n",
      "Page 14: 9 properties found\n",
      "Page 15: 9 properties found\n",
      "Page 16: 9 properties found\n",
      "Page 17: 9 properties found\n",
      "Page 18: 9 properties found\n",
      "Page 19: 9 properties found\n",
      "Page 20: 9 properties found\n",
      "Page 21: 9 properties found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 629\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    628\u001b[0m     nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m--> 629\u001b[0m     results \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(run_complete_scraper())\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\events.py:88\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:396\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:303\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    301\u001b[0m _enter_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step_run_and_handle_result(exc)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     _leave_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[6], line 556\u001b[0m, in \u001b[0;36mrun_complete_scraper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    552\u001b[0m max_for_this_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(remaining_needed, \u001b[38;5;241m500\u001b[39m)  \u001b[38;5;66;03m# Max 500 per search combination\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# Get property links first to check for duplicates\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     property_links \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mget_property_links(search_url, max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;66;03m# Filter out already scraped URLs\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     new_links \u001b[38;5;241m=\u001b[39m [link \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m property_links \u001b[38;5;28;01mif\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m scraped_urls]\n",
      "Cell \u001b[1;32mIn[6], line 118\u001b[0m, in \u001b[0;36mRightmoveRentalScraper.get_property_links\u001b[1;34m(self, search_url, max_pages)\u001b[0m\n\u001b[0;32m    116\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m24\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(page_url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m    119\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# Try multiple selectors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import jmespath\n",
    "import asyncio\n",
    "from httpx import AsyncClient\n",
    "from parsel import Selector\n",
    "import nest_asyncio\n",
    "\n",
    "class RightmoveRentalScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.properties = []\n",
    "        self.scraped_urls = set()  # Track scraped URLs to avoid duplicates\n",
    "        self.httpx_client = AsyncClient(\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            },\n",
    "            follow_redirects=True,\n",
    "            http2=False,\n",
    "            timeout=30,\n",
    "        )\n",
    "    \n",
    "    def find_json_objects(self, text: str):\n",
    "        pos = 0\n",
    "        while True:\n",
    "            match = text.find(\"{\", pos)\n",
    "            if match == -1: break\n",
    "            try:\n",
    "                result, index = json.JSONDecoder().raw_decode(text[match:])\n",
    "                yield result\n",
    "                pos = match + index\n",
    "            except:\n",
    "                pos = match + 1\n",
    "\n",
    "    def extract_property_id(self, url):\n",
    "        \"\"\"Extract property ID from URL\"\"\"\n",
    "        match = re.search(r'/properties/(\\d+)', url)\n",
    "        return match.group(1) if match else 'N/A'\n",
    "    \n",
    "    def format_address_and_postcode(self, address_data):\n",
    "        \"\"\"Extract and format address and postcode from JSON address object\"\"\"\n",
    "        if isinstance(address_data, dict):\n",
    "            display_address = address_data.get('displayAddress', '')\n",
    "            if display_address:\n",
    "                clean_address = re.sub(r'\\r\\n|\\r|\\n', ', ', display_address)\n",
    "                clean_address = re.sub(r',\\s*,', ',', clean_address)\n",
    "                clean_address = re.sub(r'\\s+', ' ', clean_address).strip()\n",
    "                postcode_pattern = r',?\\s*[A-Z]{1,2}\\d{1,2}[A-Z]?\\s*\\d[A-Z]{2}\\s*$'\n",
    "                clean_address = re.sub(postcode_pattern, '', clean_address, flags=re.IGNORECASE)\n",
    "                clean_address = clean_address.rstrip(', ')\n",
    "            else:\n",
    "                clean_address = 'N/A'\n",
    "            \n",
    "            outcode = address_data.get('outcode', '')\n",
    "            incode = address_data.get('incode', '')\n",
    "            \n",
    "            if outcode and incode:\n",
    "                postcode = f\"{outcode} {incode}\"\n",
    "            else:\n",
    "                postcode = 'N/A'\n",
    "            \n",
    "            return clean_address, postcode\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    def extract_tenure_details(self, json_data):\n",
    "        \"\"\"Extract tenure type and lease years remaining from JSON data\"\"\"\n",
    "        tenure_type = 'N/A'\n",
    "        lease_years_remaining = 'N/A'\n",
    "        \n",
    "        if json_data:\n",
    "            tenure_info = jmespath.search(\"tenure\", json_data)\n",
    "            \n",
    "            if isinstance(tenure_info, dict):\n",
    "                tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                years_remaining = tenure_info.get('yearsRemainingOnLease')\n",
    "                if years_remaining is not None:\n",
    "                    lease_years_remaining = years_remaining\n",
    "                \n",
    "                message = tenure_info.get('message')\n",
    "                if message:\n",
    "                    tenure_type = f\"{tenure_type} - {message}\"\n",
    "            \n",
    "            elif isinstance(tenure_info, str):\n",
    "                tenure_type = tenure_info\n",
    "        \n",
    "        return tenure_type, lease_years_remaining\n",
    "\n",
    "    async def extract_property_json_data(self, property_url):\n",
    "        try:\n",
    "            response = await self.httpx_client.get(property_url)\n",
    "            data = Selector(response.text).xpath(\"//script[contains(.,'PAGE_MODEL = ')]/text()\").get()\n",
    "            if data:\n",
    "                json_objects = list(self.find_json_objects(data))\n",
    "                for obj in json_objects:\n",
    "                    if \"propertyData\" in obj:\n",
    "                        return obj[\"propertyData\"]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"JSON extraction error for {property_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_property_links(self, search_url, max_pages=42):\n",
    "        \"\"\"Get property links with enhanced duplicate detection\"\"\"\n",
    "        property_links = []\n",
    "        consecutive_empty_pages = 0\n",
    "        \n",
    "        for page in range(max_pages):\n",
    "            page_url = f\"{search_url}&index={page * 24}\"\n",
    "            try:\n",
    "                response = self.session.get(page_url, timeout=15)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Try multiple selectors\n",
    "                links = []\n",
    "                selectors = [\n",
    "                    'a.propertyCard-link',\n",
    "                    'a[href*=\"/properties/\"]',\n",
    "                    '.propertyCard a',\n",
    "                    '[data-test=\"property-card\"] a'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors:\n",
    "                    found_links = soup.select(selector)\n",
    "                    if found_links:\n",
    "                        links = [f\"https://www.rightmove.co.uk{link.get('href')}\" \n",
    "                                for link in found_links if link.get('href')]\n",
    "                        break\n",
    "                \n",
    "                if len(links) == 0:\n",
    "                    consecutive_empty_pages += 1\n",
    "                    if consecutive_empty_pages >= 3:  # Stop if 3 consecutive empty pages\n",
    "                        print(f\"No more results found after page {page + 1}\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_pages = 0\n",
    "                    property_links.extend(links)\n",
    "                    print(f\"Page {page + 1}: {len(links)} properties found\")\n",
    "                \n",
    "                time.sleep(1)  # Be respectful\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page + 1}: {e}\")\n",
    "                consecutive_empty_pages += 1\n",
    "                if consecutive_empty_pages >= 3:\n",
    "                    break\n",
    "                \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_links = []\n",
    "        for link in property_links:\n",
    "            if link not in seen:\n",
    "                seen.add(link)\n",
    "                unique_links.append(link)\n",
    "        \n",
    "        return unique_links\n",
    "\n",
    "    async def scrape_property_details_fast(self, property_url):\n",
    "        # Skip if already scraped\n",
    "        if property_url in self.scraped_urls:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            html_task = asyncio.create_task(self.httpx_client.get(property_url))\n",
    "            json_data = await self.extract_property_json_data(property_url)\n",
    "            response = await html_task\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            property_data = {'url': property_url}\n",
    "            \n",
    "            # Mark as scraped\n",
    "            self.scraped_urls.add(property_url)\n",
    "            \n",
    "            # Extract property ID\n",
    "            property_id = self.extract_property_id(property_url)\n",
    "            property_data['title'] = property_id\n",
    "            \n",
    "            # Basic property info with enhanced extraction\n",
    "            all_text = soup.get_text()\n",
    "            \n",
    "            # Enhanced bedroom extraction\n",
    "            bed_patterns = [\n",
    "                r'(\\d+)\\s*bedroom',\n",
    "                r'(\\d+)\\s*bed(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bed'\n",
    "            ]\n",
    "            property_data['bedrooms'] = 'N/A'\n",
    "            for pattern in bed_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bedrooms'] = match.group(1)\n",
    "                    break\n",
    "            \n",
    "            # Enhanced bathroom extraction\n",
    "            bath_patterns = [\n",
    "                r'(\\d+)\\s*bathroom',\n",
    "                r'(\\d+)\\s*bath(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bath'\n",
    "            ]\n",
    "            property_data['bathrooms'] = 'N/A'\n",
    "            for pattern in bath_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bathrooms'] = match.group(1)\n",
    "                    break\n",
    "\n",
    "            # Key features extraction with multiple methods\n",
    "            property_data['key_features'] = 'N/A'\n",
    "            \n",
    "            # Method 1: Look for feature lists\n",
    "            feature_selectors = [\n",
    "                'ul.lIhZ24u1NHlVy5_W9V__6 li',\n",
    "                '.key-features li',\n",
    "                '[data-test=\"key-features\"] li',\n",
    "                'h2:contains(\"Key features\") + ul li',\n",
    "                'h3:contains(\"Key features\") + ul li'\n",
    "            ]\n",
    "            \n",
    "            for selector in feature_selectors:\n",
    "                features = soup.select(selector)\n",
    "                if features:\n",
    "                    feature_texts = [f.get_text().strip() for f in features]\n",
    "                    property_data['key_features'] = '; '.join(feature_texts)\n",
    "                    break\n",
    "            \n",
    "            # Method 2: Look for feature headers and following lists\n",
    "            if property_data['key_features'] == 'N/A':\n",
    "                feature_headers = soup.find_all(['h2', 'h3'], string=re.compile(r'key features|features|amenities', re.I))\n",
    "                for header in feature_headers:\n",
    "                    next_element = header.find_next_sibling()\n",
    "                    while next_element:\n",
    "                        if next_element.name in ['ul', 'ol']:\n",
    "                            features = [li.get_text().strip() for li in next_element.find_all('li')]\n",
    "                            if features:\n",
    "                                property_data['key_features'] = '; '.join(features)\n",
    "                                break\n",
    "                        next_element = next_element.find_next_sibling()\n",
    "                    if property_data['key_features'] != 'N/A':\n",
    "                        break\n",
    "            \n",
    "            # Property details extraction\n",
    "            property_data.update({\n",
    "                'parking': 'N/A',\n",
    "                'garden': 'N/A',\n",
    "                'council_tax': 'N/A',\n",
    "                'accessibility': 'N/A',\n",
    "                'size_sqft': 'N/A',\n",
    "                'size_sqm': 'N/A',\n",
    "                'furnish_status': 'N/A',\n",
    "            })\n",
    "           \n",
    "            # Extract details from various selectors\n",
    "            detail_selectors = [\n",
    "                'dt._17A0LehXZKxGHbPeiLQ1BI',\n",
    "                'dt[class*=\"detail\"]',\n",
    "                '.property-details dt',\n",
    "                'dl dt'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                detail_sections = soup.select(selector)\n",
    "                if detail_sections:\n",
    "                    for section in detail_sections:\n",
    "                        section_text = section.get_text().strip().upper()\n",
    "                        value_element = section.find_next_sibling(['dd', 'span', 'div'])\n",
    "                        value_text = value_element.get_text().strip() if value_element else 'N/A'\n",
    "                        \n",
    "                        if 'PARKING' in section_text:\n",
    "                            property_data['parking'] = value_text\n",
    "                        elif 'GARDEN' in section_text:\n",
    "                            property_data['garden'] = value_text\n",
    "                        elif 'COUNCIL TAX' in section_text:\n",
    "                            property_data['council_tax'] = value_text\n",
    "                        elif 'ACCESSIBILITY' in section_text:\n",
    "                            property_data['accessibility'] = value_text\n",
    "                        elif 'FURNISH' in section_text:\n",
    "                            property_data['furnish_status'] = value_text\n",
    "                    break\n",
    "\n",
    "            # Size extraction\n",
    "            size_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*ft',\n",
    "                r'([0-9,]+)\\s*sqft',\n",
    "                r'([0-9,]+)\\s*square\\s*feet'\n",
    "            ]\n",
    "            for pattern in size_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqft'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "            \n",
    "            sqm_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*m',\n",
    "                r'([0-9,]+)\\s*sqm',\n",
    "                r'([0-9,]+)\\s*square\\s*metres?'\n",
    "            ]\n",
    "            for pattern in sqm_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqm'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "\n",
    "            # Extract data from JSON if available\n",
    "            if json_data:\n",
    "                try:\n",
    "                    stations = jmespath.search(\"nearestStations[*].{name: name, distance: distance}\", json_data) or []\n",
    "                    photos = jmespath.search(\"images[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    floorplans = jmespath.search(\"floorplans[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    property_type = (jmespath.search(\"propertySubType\", json_data) or \n",
    "                                   jmespath.search(\"propertyType\", json_data) or 'N/A')\n",
    "                    description = jmespath.search(\"text.description\", json_data) or 'N/A'\n",
    "                    address_data = jmespath.search(\"address\", json_data)\n",
    "                    latitude = jmespath.search(\"location.latitude\", json_data) or 'N/A'\n",
    "                    longitude = jmespath.search(\"location.longitude\", json_data) or 'N/A'\n",
    "                    price = jmespath.search(\"prices.primaryPrice\", json_data) or 'N/A'\n",
    "                    \n",
    "                    # Tenure information\n",
    "                    tenure_info = jmespath.search(\"tenure\", json_data) or {}\n",
    "                    tenure_type = 'N/A'\n",
    "                    lease_years_remaining = 'N/A'\n",
    "                    \n",
    "                    if isinstance(tenure_info, dict):\n",
    "                        tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                        lease_years_remaining = tenure_info.get('yearsRemainingOnLease', 'N/A')\n",
    "                    elif isinstance(tenure_info, str):\n",
    "                        tenure_type = tenure_info\n",
    "                    \n",
    "                    formatted_address, postcode = self.format_address_and_postcode(address_data)\n",
    "                    \n",
    "                    property_data.update({\n",
    "                        'nearest_stations': '; '.join([s['name'] for s in stations]) or 'N/A',\n",
    "                        'station_distances': '; '.join([f\"{s['distance']} miles\" for s in stations]) or 'N/A',\n",
    "                        'station_count': len(stations),\n",
    "                        'image_count': len(photos),\n",
    "                        'image_urls': '; '.join([p['url'] for p in photos]) or 'N/A',\n",
    "                        'floorplan_count': len(floorplans),\n",
    "                        'floorplan_urls': '; '.join([f['url'] for f in floorplans]) or 'N/A',\n",
    "                        'property_type': property_type,\n",
    "                        'tenure_type': tenure_type,\n",
    "                        'lease_years_remaining': lease_years_remaining,\n",
    "                        'description': description,\n",
    "                        'latitude': latitude,\n",
    "                        'longitude': longitude,\n",
    "                        'address': formatted_address,\n",
    "                        'postcode': postcode,\n",
    "                        'price': price\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting JSON data: {e}\")\n",
    "            else:\n",
    "                # HTML fallback\n",
    "                property_type_fallback = 'N/A'\n",
    "                type_element = soup.find(string=re.compile(r'(flat|house|apartment|studio|maisonette)', re.I))\n",
    "                if type_element:\n",
    "                    property_type_fallback = type_element.strip()\n",
    "                \n",
    "                property_data.update({\n",
    "                    'nearest_stations': 'N/A',\n",
    "                    'station_distances': 'N/A', \n",
    "                    'station_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'image_urls': 'N/A',\n",
    "                    'floorplan_count': 0,\n",
    "                    'floorplan_urls': 'N/A',\n",
    "                    'property_type': property_type_fallback,\n",
    "                    'description': 'N/A',\n",
    "                    'tenure_type': 'N/A',\n",
    "                    'lease_years_remaining': 'N/A',\n",
    "                    'latitude': 'N/A',\n",
    "                    'longitude': 'N/A',\n",
    "                    'address': 'N/A',\n",
    "                    'postcode': 'N/A',\n",
    "                    'price': 'N/A'\n",
    "                })\n",
    "            \n",
    "            return property_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {property_url}: {e}\")\n",
    "            return {'url': property_url, 'error': str(e)}\n",
    "\n",
    "    async def scrape_all_properties_fast(self, search_url, max_pages=42, max_properties=None):\n",
    "        print(f\"Getting property links from up to {max_pages} pages...\")\n",
    "        property_links = self.get_property_links(search_url, max_pages)\n",
    "        \n",
    "        if max_properties:\n",
    "            property_links = property_links[:max_properties]\n",
    "        \n",
    "        print(f\"Found {len(property_links)} unique properties to scrape...\")\n",
    "        \n",
    "        if not property_links:\n",
    "            print(\"No property links found.\")\n",
    "            return\n",
    "        \n",
    "        # Filter out already scraped properties\n",
    "        new_links = [link for link in property_links if link not in self.scraped_urls]\n",
    "        print(f\"New properties to scrape: {len(new_links)}\")\n",
    "        \n",
    "        batch_size = 3  # Conservative batch size\n",
    "        successful = 0\n",
    "        \n",
    "        for i in range(0, len(new_links), batch_size):\n",
    "            batch = new_links[i:i+batch_size]\n",
    "            tasks = [self.scrape_property_details_fast(url) for url in batch]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            for result in results:\n",
    "                if isinstance(result, dict) and result is not None and 'error' not in result:\n",
    "                    self.properties.append(result)\n",
    "                    successful += 1\n",
    "                elif isinstance(result, dict) and 'error' in result:\n",
    "                    print(f\"Error in result: {result['error']}\")\n",
    "            \n",
    "            print(f\"Completed {min(i+batch_size, len(new_links))}/{len(new_links)} properties (Success: {successful})\")\n",
    "            await asyncio.sleep(1)  # Rate limiting\n",
    "\n",
    "    def save_to_csv(self, filename='rightmove_all_properties.csv'):\n",
    "        if not self.properties:\n",
    "            print(\"No properties to save!\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(self.properties)\n",
    "        \n",
    "        # Clean description field\n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].astype(str).apply(\n",
    "                lambda x: re.sub(r'<[^>]+>', ' ', x) if x != 'N/A' else x\n",
    "            ).apply(\n",
    "                lambda x: re.sub(r'\\s+', ' ', x).strip() if x != 'N/A' else x\n",
    "            )\n",
    "        \n",
    "        # Remove duplicate properties by URL\n",
    "        df_clean = df.drop_duplicates(subset=['url'], keep='first')\n",
    "        \n",
    "        df_clean.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df_clean)} unique properties to {filename}\")\n",
    "        \n",
    "        if len(df_clean) > 0:\n",
    "            print(f\"\\nData Summary:\")\n",
    "            print(f\"Properties with prices: {len(df_clean[df_clean['price'] != 'N/A'])}\")\n",
    "            print(f\"Properties with addresses: {len(df_clean[df_clean['address'] != 'N/A'])}\")\n",
    "            print(f\"Properties with images: {len(df_clean[df_clean['image_count'] != 0])}\")\n",
    "        \n",
    "        return df_clean\n",
    "\n",
    "        # ...existing code...\n",
    "\n",
    "    # ...existing code...\n",
    "\n",
    "async def run_complete_scraper():\n",
    "    scraper = RightmoveRentalScraper()\n",
    "    \n",
    "    # London Boroughs with their proper location identifiers\n",
    "    london_boroughs = [\n",
    "        # Inner London Boroughs\n",
    "        {\"name\": \"City of London\", \"id\": \"REGION%5E92824\"},\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Enhanced price ranges to segment large result sets\n",
    "    price_ranges = [\n",
    "        \"\",  # All prices (no price filter)\n",
    "        \n",
    "        # Lower price ranges\n",
    "        \"&maxPrice=230000\",\n",
    "        \"&minPrice=230000&maxPrice=240000\",\n",
    "        \"&minPrice=240000&maxPrice=250000\",\n",
    "        \"&minPrice=250000&maxPrice=260000\",\n",
    "        \"&minPrice=260000&maxPrice=270000\",\n",
    "        \"&minPrice=270000&maxPrice=280000\",\n",
    "        \"&minPrice=280000&maxPrice=290000\",\n",
    "        \"&minPrice=290000&maxPrice=300000\",\n",
    "        \"&minPrice=300000&maxPrice=325000\",\n",
    "        \"&minPrice=325000&maxPrice=350000\",\n",
    "        \"&minPrice=350000&maxPrice=375000\",\n",
    "        \"&minPrice=375000&maxPrice=400000\",\n",
    "        \"&minPrice=400000&maxPrice=425000\",\n",
    "        \"&minPrice=425000&maxPrice=450000\",\n",
    "        \"&minPrice=450000&maxPrice=475000\",\n",
    "        \"&minPrice=475000&maxPrice=500000\",\n",
    "        \n",
    "        # Mid-range prices\n",
    "        \"&minPrice=500000&maxPrice=550000\",\n",
    "        \"&minPrice=550000&maxPrice=600000\",\n",
    "        \"&minPrice=600000&maxPrice=650000\",\n",
    "        \"&minPrice=650000&maxPrice=700000\",\n",
    "        \"&minPrice=700000&maxPrice=800000\",\n",
    "        \"&minPrice=800000&maxPrice=900000\",\n",
    "        \"&minPrice=900000&maxPrice=1000000\",\n",
    "        \n",
    "        # Higher price ranges\n",
    "        \"&minPrice=1000000&maxPrice=1250000\",\n",
    "        \"&minPrice=1250000&maxPrice=1500000\",\n",
    "        \"&minPrice=1500000&maxPrice=1750000\",\n",
    "        \"&minPrice=1750000&maxPrice=2000000\",\n",
    "        \"&minPrice=2000000&maxPrice=2500000\",\n",
    "        \"&minPrice=2500000&maxPrice=3000000\",\n",
    "        \"&minPrice=3000000&maxPrice=4000000\",\n",
    "        \"&minPrice=4000000&maxPrice=5000000\",\n",
    "        \"&minPrice=5000000&maxPrice=7500000\",\n",
    "        \"&minPrice=7500000&maxPrice=10000000\",\n",
    "        \"&minPrice=10000000&maxPrice=15000000\",\n",
    "        \"&minPrice=15000000&maxPrice=20000000\",\n",
    "        \n",
    "        # Ultra-high end (no maximum)\n",
    "        \"&minPrice=20000000\"\n",
    "    ]\n",
    "    \n",
    "    # Generate all search URL combinations with proper format\n",
    "    london_search_urls = []\n",
    "    \n",
    "    for borough in london_boroughs:\n",
    "        for price_range in price_ranges:\n",
    "            # Sale properties with proper URL format\n",
    "            sale_url = f\"https://www.rightmove.co.uk/property-for-sale/find.html?useLocationIdentifier=true&locationIdentifier={borough['id']}&buy=For+sale&radius=0.0&_includeSSTC=on&index=0&sortType=2&channel=BUY&transactionType=BUY&displayLocationIdentifier={borough['name'].replace(' ', '-')}.html{price_range}\"\n",
    "            london_search_urls.append({\n",
    "                'url': sale_url,\n",
    "                'name': borough['name'],\n",
    "                'type': 'sale',\n",
    "                'price_range': price_range.replace('&', '') if price_range else 'all_prices'\n",
    "            })\n",
    "    \n",
    "    print(f\"Generated {len(london_search_urls)} London search combinations\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    target_properties = 50000\n",
    "    scraped_urls = set()  # Track to avoid duplicates\n",
    "    \n",
    "    for i, search_item in enumerate(london_search_urls):\n",
    "        search_url = search_item['url']\n",
    "        borough_name = search_item['name']\n",
    "        search_type = search_item['type']\n",
    "        price_range = search_item['price_range']\n",
    "        \n",
    "        print(f\"\\n🏙️ Scraping {borough_name} ({search_type}) - {price_range}\")\n",
    "        print(f\"Current total: {len(scraper.properties)} properties\")\n",
    "        \n",
    "        # Calculate how many more properties we need\n",
    "        remaining_needed = target_properties - len(scraper.properties)\n",
    "        if remaining_needed <= 0:\n",
    "            print(f\"✅ Target of {target_properties} properties reached!\")\n",
    "            break\n",
    "        \n",
    "        # Set max_properties for this search (limit per search combination)\n",
    "        max_for_this_search = min(remaining_needed, 500)  # Max 500 per search combination\n",
    "        \n",
    "        try:\n",
    "            # Get property links first to check for duplicates\n",
    "            property_links = scraper.get_property_links(search_url, max_pages=42)\n",
    "            \n",
    "            # Filter out already scraped URLs\n",
    "            new_links = [link for link in property_links if link not in scraped_urls]\n",
    "            \n",
    "            if new_links:\n",
    "                # Limit to max_for_this_search\n",
    "                new_links = new_links[:max_for_this_search]\n",
    "                \n",
    "                # Add to scraped set\n",
    "                scraped_urls.update(new_links)\n",
    "                \n",
    "                print(f\"Found {len(new_links)} new properties to scrape\")\n",
    "                \n",
    "                # Scrape the new properties\n",
    "                batch_size = 5\n",
    "                for j in range(0, len(new_links), batch_size):\n",
    "                    batch = new_links[j:j+batch_size]\n",
    "                    tasks = [scraper.scrape_property_details_fast(url) for url in batch]\n",
    "                    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "                    \n",
    "                    for result in results:\n",
    "                        if isinstance(result, dict) and result is not None and 'error' not in result:\n",
    "                            scraper.properties.append(result)\n",
    "                    \n",
    "                    print(f\"Completed {min(j+batch_size, len(new_links))}/{len(new_links)} properties from {borough_name}\")\n",
    "                    await asyncio.sleep(0.5)\n",
    "            else:\n",
    "                print(f\"No new properties found in {borough_name} - {price_range}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {borough_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"✅ {borough_name} ({price_range}) completed. Total properties: {len(scraper.properties)}\")\n",
    "        \n",
    "        # Break if we've reached our target\n",
    "        if len(scraper.properties) >= target_properties:\n",
    "            print(f\"🎯 Target reached! Total: {len(scraper.properties)} properties\")\n",
    "            break\n",
    "        \n",
    "        # Small delay between searches\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Trim to exactly 50,000 if we got more\n",
    "    if len(scraper.properties) > target_properties:\n",
    "        scraper.properties = scraper.properties[:target_properties]\n",
    "        print(f\"✂️ Trimmed to exactly {target_properties} properties\")\n",
    "    \n",
    "    # Remove any duplicate properties by URL\n",
    "    seen_urls = set()\n",
    "    unique_properties = []\n",
    "    for prop in scraper.properties:\n",
    "        if prop.get('url') not in seen_urls:\n",
    "            seen_urls.add(prop.get('url'))\n",
    "            unique_properties.append(prop)\n",
    "    \n",
    "    scraper.properties = unique_properties\n",
    "    \n",
    "    scraper.save_to_csv('rightmove_london_50k_comprehensive.csv')\n",
    "    await scraper.httpx_client.aclose()\n",
    "    \n",
    "    print(f\"⚡ Completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"📊 Final count: {len(scraper.properties)} unique London properties\")\n",
    "    print(f\"🏘️ Covered all {len(london_boroughs)} London boroughs/areas\")\n",
    "    return scraper.properties\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    results = asyncio.run(run_complete_scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bfdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 35 London search combinations\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=200000maxPrice=300000\n",
      "Current total: 0 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 1 properties found\n",
      "Page 2: 1 properties found\n",
      "Page 3: 1 properties found\n",
      "Page 4: 1 properties found\n",
      "Page 5: 1 properties found\n",
      "Page 6: 1 properties found\n",
      "Page 7: 1 properties found\n",
      "Page 8: 1 properties found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3598' coro=<run_complete_scraper() done, defined at C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3331485428.py:457> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3331485428.py\", line 629, in <module>\n",
      "    results = asyncio.run(run_complete_scraper())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py\", line 396, in __wakeup\n",
      "    self.__step()\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3331485428.py\", line 556, in run_complete_scraper\n",
      "    property_links = scraper.get_property_links(search_url, max_pages=42)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3331485428.py\", line 118, in get_property_links\n",
      "    response = self.session.get(page_url, timeout=15)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\adapters.py\", line 589, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\socket.py\", line 708, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9: 1 properties found\n",
      "Page 10: 1 properties found\n",
      "Page 11: 1 properties found\n",
      "Page 12: 1 properties found\n",
      "Page 13: 1 properties found\n",
      "Page 14: 1 properties found\n",
      "Page 15: 1 properties found\n",
      "Page 16: 1 properties found\n",
      "Page 17: 1 properties found\n",
      "Page 18: 1 properties found\n",
      "Page 19: 1 properties found\n",
      "Page 20: 1 properties found\n",
      "Page 21: 1 properties found\n",
      "Page 22: 1 properties found\n",
      "Page 23: 1 properties found\n",
      "Page 24: 1 properties found\n",
      "Page 25: 1 properties found\n",
      "Page 26: 1 properties found\n",
      "Page 27: 1 properties found\n",
      "Page 28: 1 properties found\n",
      "Page 29: 1 properties found\n",
      "Page 30: 1 properties found\n",
      "Page 31: 1 properties found\n",
      "Page 32: 1 properties found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 623\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    622\u001b[0m     nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m--> 623\u001b[0m     results \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(run_complete_scraper())\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\events.py:88\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:303\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    301\u001b[0m _enter_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step_run_and_handle_result(exc)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     _leave_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[8], line 576\u001b[0m, in \u001b[0;36mrun_complete_scraper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    572\u001b[0m max_for_this_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(remaining_needed, \u001b[38;5;241m300\u001b[39m)  \u001b[38;5;66;03m# Reduced from 500 since we have more granular searches\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Smart page detection - will stop early if no results\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mscrape_all_properties_fast(\n\u001b[0;32m    577\u001b[0m         search_url, \n\u001b[0;32m    578\u001b[0m         max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,  \u001b[38;5;66;03m# Max possible, but will stop early\u001b[39;00m\n\u001b[0;32m    579\u001b[0m         max_properties\u001b[38;5;241m=\u001b[39mmax_for_this_search\n\u001b[0;32m    580\u001b[0m     )\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError scraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborough_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 412\u001b[0m, in \u001b[0;36mRightmoveRentalScraper.scrape_all_properties_fast\u001b[1;34m(self, search_url, max_pages, max_properties)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_all_properties_fast\u001b[39m(\u001b[38;5;28mself\u001b[39m, search_url, max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_properties\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting property links (will stop early if no results found)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 412\u001b[0m     property_links \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_property_links(search_url, max_pages)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_properties:\n\u001b[0;32m    415\u001b[0m         property_links \u001b[38;5;241m=\u001b[39m property_links[:max_properties]\n",
      "Cell \u001b[1;32mIn[8], line 130\u001b[0m, in \u001b[0;36mRightmoveRentalScraper.get_property_links\u001b[1;34m(self, search_url, max_pages)\u001b[0m\n\u001b[0;32m    128\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m24\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(page_url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m    131\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Try multiple selectors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import jmespath\n",
    "import asyncio\n",
    "from httpx import AsyncClient\n",
    "from parsel import Selector\n",
    "import nest_asyncio\n",
    "\n",
    "class RightmoveRentalScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.properties = []\n",
    "        self.scraped_urls = set()  # Track scraped URLs to avoid duplicates\n",
    "        self.httpx_client = AsyncClient(\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            },\n",
    "            follow_redirects=True,\n",
    "            http2=False,\n",
    "            timeout=30,\n",
    "        )\n",
    "    \n",
    "    def find_json_objects(self, text: str):\n",
    "        pos = 0\n",
    "        while True:\n",
    "            match = text.find(\"{\", pos)\n",
    "            if match == -1: break\n",
    "            try:\n",
    "                result, index = json.JSONDecoder().raw_decode(text[match:])\n",
    "                yield result\n",
    "                pos = match + index\n",
    "            except:\n",
    "                pos = match + 1\n",
    "\n",
    "    def extract_property_id(self, url):\n",
    "        \"\"\"Extract property ID from URL\"\"\"\n",
    "        match = re.search(r'/properties/(\\d+)', url)\n",
    "        return match.group(1) if match else 'N/A'\n",
    "    \n",
    "    def format_address_and_postcode(self, address_data):\n",
    "        \"\"\"Extract and format address and postcode from JSON address object\"\"\"\n",
    "        if isinstance(address_data, dict):\n",
    "            # Extract display address and clean it\n",
    "            display_address = address_data.get('displayAddress', '')\n",
    "            if display_address:\n",
    "                # Remove line breaks and extra spaces\n",
    "                clean_address = re.sub(r'\\r\\n|\\r|\\n', ', ', display_address)\n",
    "                clean_address = re.sub(r',\\s*,', ',', clean_address)  # Remove double commas\n",
    "                clean_address = re.sub(r'\\s+', ' ', clean_address).strip()  # Normalize spaces\n",
    "                \n",
    "                # Remove postcode from address if it appears at the end\n",
    "                postcode_pattern = r',?\\s*[A-Z]{1,2}\\d{1,2}[A-Z]?\\s*\\d[A-Z]{2}\\s*$'\n",
    "                clean_address = re.sub(postcode_pattern, '', clean_address, flags=re.IGNORECASE)\n",
    "                clean_address = clean_address.rstrip(', ')\n",
    "            else:\n",
    "                clean_address = 'N/A'\n",
    "            \n",
    "            # Combine outcode and incode to form postcode\n",
    "            outcode = address_data.get('outcode', '')\n",
    "            incode = address_data.get('incode', '')\n",
    "            \n",
    "            if outcode and incode:\n",
    "                postcode = f\"{outcode} {incode}\"\n",
    "            else:\n",
    "                postcode = 'N/A'\n",
    "            \n",
    "            return clean_address, postcode\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    def extract_tenure_details(self, json_data):\n",
    "        \"\"\"Extract tenure type and lease years remaining from JSON data\"\"\"\n",
    "        tenure_type = 'N/A'\n",
    "        lease_years_remaining = 'N/A'\n",
    "        \n",
    "        if json_data:\n",
    "            # Extract tenure information\n",
    "            tenure_info = jmespath.search(\"tenure\", json_data)\n",
    "            \n",
    "            if isinstance(tenure_info, dict):\n",
    "                # Extract tenure type\n",
    "                tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                \n",
    "                # Extract years remaining on lease\n",
    "                years_remaining = tenure_info.get('yearsRemainingOnLease')\n",
    "                if years_remaining is not None:\n",
    "                    lease_years_remaining = years_remaining\n",
    "                \n",
    "                # Handle message field if needed\n",
    "                message = tenure_info.get('message')\n",
    "                if message:\n",
    "                    # You can append message to tenure_type if it contains useful info\n",
    "                    tenure_type = f\"{tenure_type} - {message}\"\n",
    "            \n",
    "            elif isinstance(tenure_info, str):\n",
    "                # If it's just a string, use it as tenure type\n",
    "                tenure_type = tenure_info\n",
    "        \n",
    "        return tenure_type, lease_years_remaining\n",
    "\n",
    "    async def extract_property_json_data(self, property_url):\n",
    "        try:\n",
    "            response = await self.httpx_client.get(property_url)\n",
    "            data = Selector(response.text).xpath(\"//script[contains(.,'PAGE_MODEL = ')]/text()\").get()\n",
    "            if data:\n",
    "                json_objects = list(self.find_json_objects(data))\n",
    "                for obj in json_objects:\n",
    "                    if \"propertyData\" in obj:\n",
    "                        return obj[\"propertyData\"]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"JSON extraction error for {property_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_property_links(self, search_url, max_pages=42):\n",
    "        \"\"\"Get property links with smart page detection - stops early if no results\"\"\"\n",
    "        property_links = []\n",
    "        consecutive_empty_pages = 0\n",
    "        \n",
    "        for page in range(max_pages):\n",
    "            page_url = f\"{search_url}&index={page * 24}\"\n",
    "            try:\n",
    "                response = self.session.get(page_url, timeout=15)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Try multiple selectors\n",
    "                links = []\n",
    "                selectors = [\n",
    "                    'a.propertyCard-link',\n",
    "                    'a[href*=\"/properties/\"]',\n",
    "                    '.propertyCard a',\n",
    "                    '[data-test=\"property-card\"] a'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors:\n",
    "                    found_links = soup.select(selector)\n",
    "                    if found_links:\n",
    "                        links = [f\"https://www.rightmove.co.uk{link.get('href')}\" \n",
    "                                for link in found_links if link.get('href')]\n",
    "                        break\n",
    "                \n",
    "                if len(links) == 0:\n",
    "                    consecutive_empty_pages += 1\n",
    "                    print(f\"Page {page + 1}: 0 properties found\")\n",
    "                    # Stop after 2 consecutive empty pages (faster than 3)\n",
    "                    if consecutive_empty_pages >= 2:\n",
    "                        print(f\"No more results found after page {page + 1}\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_pages = 0\n",
    "                    property_links.extend(links)\n",
    "                    print(f\"Page {page + 1}: {len(links)} properties found\")\n",
    "                \n",
    "                time.sleep(0.5)  # Reduced delay for faster scraping\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page + 1}: {e}\")\n",
    "                consecutive_empty_pages += 1\n",
    "                if consecutive_empty_pages >= 2:\n",
    "                    break\n",
    "                \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_links = []\n",
    "        for link in property_links:\n",
    "            if link not in seen:\n",
    "                seen.add(link)\n",
    "                unique_links.append(link)\n",
    "        \n",
    "        return unique_links\n",
    "\n",
    "    async def scrape_property_details_fast(self, property_url):\n",
    "        # Skip if already scraped\n",
    "        if property_url in self.scraped_urls:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            html_task = asyncio.create_task(self.httpx_client.get(property_url))\n",
    "            json_data = await self.extract_property_json_data(property_url)\n",
    "            response = await html_task\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            property_data = {'url': property_url}\n",
    "            \n",
    "            # Mark as scraped\n",
    "            self.scraped_urls.add(property_url)\n",
    "            \n",
    "            # Extract property ID and use as title\n",
    "            property_id = self.extract_property_id(property_url)\n",
    "            property_data['title'] = property_id\n",
    "            \n",
    "            # Basic property info with enhanced extraction\n",
    "            all_text = soup.get_text()\n",
    "            \n",
    "            # Enhanced bedroom extraction\n",
    "            bed_patterns = [\n",
    "                r'(\\d+)\\s*bedroom',\n",
    "                r'(\\d+)\\s*bed(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bed'\n",
    "            ]\n",
    "            property_data['bedrooms'] = 'N/A'\n",
    "            for pattern in bed_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bedrooms'] = match.group(1)\n",
    "                    break\n",
    "            \n",
    "            # Enhanced bathroom extraction\n",
    "            bath_patterns = [\n",
    "                r'(\\d+)\\s*bathroom',\n",
    "                r'(\\d+)\\s*bath(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bath'\n",
    "            ]\n",
    "            property_data['bathrooms'] = 'N/A'\n",
    "            for pattern in bath_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bathrooms'] = match.group(1)\n",
    "                    break\n",
    "\n",
    "            # Key features extraction with multiple methods\n",
    "            property_data['key_features'] = 'N/A'\n",
    "            \n",
    "            # Method 1: Look for feature lists\n",
    "            feature_selectors = [\n",
    "                'ul.lIhZ24u1NHlVy5_W9V__6 li',\n",
    "                '.key-features li',\n",
    "                '[data-test=\"key-features\"] li',\n",
    "                'h2:contains(\"Key features\") + ul li',\n",
    "                'h3:contains(\"Key features\") + ul li'\n",
    "            ]\n",
    "            \n",
    "            for selector in feature_selectors:\n",
    "                features = soup.select(selector)\n",
    "                if features:\n",
    "                    feature_texts = [f.get_text().strip() for f in features]\n",
    "                    property_data['key_features'] = '; '.join(feature_texts)\n",
    "                    break\n",
    "            \n",
    "            # Method 2: Look for feature headers and following lists\n",
    "            if property_data['key_features'] == 'N/A':\n",
    "                feature_headers = soup.find_all(['h2', 'h3'], string=re.compile(r'key features|features|amenities', re.I))\n",
    "                for header in feature_headers:\n",
    "                    next_element = header.find_next_sibling()\n",
    "                    while next_element:\n",
    "                        if next_element.name in ['ul', 'ol']:\n",
    "                            features = [li.get_text().strip() for li in next_element.find_all('li')]\n",
    "                            if features:\n",
    "                                property_data['key_features'] = '; '.join(features)\n",
    "                                break\n",
    "                        next_element = next_element.find_next_sibling()\n",
    "                    if property_data['key_features'] != 'N/A':\n",
    "                        break\n",
    "            \n",
    "            # Property details extraction\n",
    "            property_data.update({\n",
    "                'parking': 'N/A',\n",
    "                'garden': 'N/A',\n",
    "                'council_tax': 'N/A',\n",
    "                'accessibility': 'N/A',\n",
    "                'size_sqft': 'N/A',\n",
    "                'size_sqm': 'N/A',\n",
    "                'furnish_status': 'N/A',\n",
    "            })\n",
    "           \n",
    "            # Extract details from various selectors\n",
    "            detail_selectors = [\n",
    "                'dt._17A0LehXZKxGHbPeiLQ1BI',\n",
    "                'dt[class*=\"detail\"]',\n",
    "                '.property-details dt',\n",
    "                'dl dt'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                detail_sections = soup.select(selector)\n",
    "                if detail_sections:\n",
    "                    for section in detail_sections:\n",
    "                        section_text = section.get_text().strip().upper()\n",
    "                        value_element = section.find_next_sibling(['dd', 'span', 'div'])\n",
    "                        value_text = value_element.get_text().strip() if value_element else 'N/A'\n",
    "                        \n",
    "                        if 'PARKING' in section_text:\n",
    "                            property_data['parking'] = value_text\n",
    "                        elif 'GARDEN' in section_text:\n",
    "                            property_data['garden'] = value_text\n",
    "                        elif 'COUNCIL TAX' in section_text:\n",
    "                            property_data['council_tax'] = value_text\n",
    "                        elif 'ACCESSIBILITY' in section_text:\n",
    "                            property_data['accessibility'] = value_text\n",
    "                        elif 'FURNISH' in section_text:\n",
    "                            property_data['furnish_status'] = value_text\n",
    "                    break\n",
    "\n",
    "            # Additional furnish status extraction\n",
    "            furnish_dt = soup.find('dt', string=re.compile(r'^Furnish type:\\s*$', re.I))\n",
    "            if furnish_dt:\n",
    "                furnish_dd = furnish_dt.find_next_sibling('dd')\n",
    "                if furnish_dd:\n",
    "                    property_data['furnish_status'] = furnish_dd.get_text().strip()\n",
    "\n",
    "            # Size extraction from full text\n",
    "            sqft_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*ft',\n",
    "                r'([0-9,]+)\\s*sqft',\n",
    "                r'([0-9,]+)\\s*square\\s*feet'\n",
    "            ]\n",
    "            for pattern in sqft_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqft'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "            \n",
    "            sqm_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*m',\n",
    "                r'([0-9,]+)\\s*sqm',\n",
    "                r'([0-9,]+)\\s*square\\s*metres?'\n",
    "            ]\n",
    "            for pattern in sqm_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqm'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "\n",
    "            # Extract data from JSON if available\n",
    "            if json_data:\n",
    "                try:\n",
    "                    stations = jmespath.search(\"nearestStations[*].{name: name, distance: distance}\", json_data) or []\n",
    "                    photos = jmespath.search(\"images[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    floorplans = jmespath.search(\"floorplans[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    property_type = (jmespath.search(\"propertySubType\", json_data) or \n",
    "                                   jmespath.search(\"propertyType\", json_data) or 'N/A')\n",
    "                    description = jmespath.search(\"text.description\", json_data) or 'N/A'\n",
    "                    address_data = jmespath.search(\"address\", json_data)\n",
    "                    latitude = jmespath.search(\"location.latitude\", json_data) or 'N/A'\n",
    "                    longitude = jmespath.search(\"location.longitude\", json_data) or 'N/A'\n",
    "                    price = jmespath.search(\"prices.primaryPrice\", json_data) or 'N/A'\n",
    "                    \n",
    "                    # Tenure information\n",
    "                    tenure_info = jmespath.search(\"tenure\", json_data) or {}\n",
    "                    tenure_type = 'N/A'\n",
    "                    lease_years_remaining = 'N/A'\n",
    "                    \n",
    "                    if isinstance(tenure_info, dict):\n",
    "                        tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                        lease_years_remaining = tenure_info.get('yearsRemainingOnLease', 'N/A')\n",
    "                    elif isinstance(tenure_info, str):\n",
    "                        tenure_type = tenure_info\n",
    "                    \n",
    "                    formatted_address, postcode = self.format_address_and_postcode(address_data)\n",
    "                    \n",
    "                    property_data.update({\n",
    "                        'nearest_stations': '; '.join([s['name'] for s in stations]) or 'N/A',\n",
    "                        'station_distances': '; '.join([f\"{s['distance']} miles\" for s in stations]) or 'N/A',\n",
    "                        'station_count': len(stations),\n",
    "                        'image_count': len(photos),\n",
    "                        'image_urls': '; '.join([p['url'] for p in photos]) or 'N/A',\n",
    "                        'floorplan_count': len(floorplans),\n",
    "                        'floorplan_urls': '; '.join([f['url'] for f in floorplans]) or 'N/A',\n",
    "                        'property_type': property_type,\n",
    "                        'tenure_type': tenure_type,\n",
    "                        'lease_years_remaining': lease_years_remaining,\n",
    "                        'description': description,\n",
    "                        'latitude': latitude,\n",
    "                        'longitude': longitude,\n",
    "                        'address': formatted_address,\n",
    "                        'postcode': postcode,\n",
    "                        'price': price\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting JSON data: {e}\")\n",
    "            else:\n",
    "                # HTML fallback for property type if JSON fails\n",
    "                property_type_fallback = 'N/A'\n",
    "                type_element = soup.find(string=re.compile(r'(flat|house|apartment|studio|maisonette)', re.I))\n",
    "                if type_element:\n",
    "                    property_type_fallback = type_element.strip()\n",
    "                \n",
    "                property_data.update({\n",
    "                    'nearest_stations': 'N/A',\n",
    "                    'station_distances': 'N/A', \n",
    "                    'station_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'image_urls': 'N/A',\n",
    "                    'floorplan_count': 0,\n",
    "                    'floorplan_urls': 'N/A',\n",
    "                    'property_type': property_type_fallback,\n",
    "                    'description': 'N/A',\n",
    "                    'tenure_type': 'N/A',\n",
    "                    'lease_years_remaining': 'N/A',\n",
    "                    'latitude': 'N/A',\n",
    "                    'longitude': 'N/A',\n",
    "                    'address': 'N/A',\n",
    "                    'postcode': 'N/A',\n",
    "                    'price': 'N/A'\n",
    "                })\n",
    "            \n",
    "            return property_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {property_url}: {e}\")\n",
    "            return {'url': property_url, 'error': str(e)}\n",
    "\n",
    "    async def scrape_all_properties_fast(self, search_url, max_pages=42, max_properties=None):\n",
    "        print(f\"Getting property links (will stop early if no results found)...\")\n",
    "        property_links = self.get_property_links(search_url, max_pages)\n",
    "        \n",
    "        if max_properties:\n",
    "            property_links = property_links[:max_properties]\n",
    "        \n",
    "        print(f\"Found {len(property_links)} unique properties to scrape...\")\n",
    "        \n",
    "        if not property_links:\n",
    "            print(\"No property links found.\")\n",
    "            return\n",
    "        \n",
    "        # Filter out already scraped properties\n",
    "        new_links = [link for link in property_links if link not in self.scraped_urls]\n",
    "        print(f\"New properties to scrape: {len(new_links)}\")\n",
    "        \n",
    "        batch_size = 3  # Conservative batch size\n",
    "        successful = 0\n",
    "        \n",
    "        for i in range(0, len(new_links), batch_size):\n",
    "            batch = new_links[i:i+batch_size]\n",
    "            tasks = [self.scrape_property_details_fast(url) for url in batch]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            for result in results:\n",
    "                if isinstance(result, dict) and result is not None and 'error' not in result:\n",
    "                    self.properties.append(result)\n",
    "                    successful += 1\n",
    "                elif isinstance(result, dict) and 'error' in result:\n",
    "                    print(f\"Error in result: {result['error']}\")\n",
    "            \n",
    "            print(f\"Completed {min(i+batch_size, len(new_links))}/{len(new_links)} properties (Success: {successful})\")\n",
    "            await asyncio.sleep(0.5)  # Reduced delay\n",
    "\n",
    "    def save_to_csv(self, filename='rightmove_london_50k_comprehensive.csv'):\n",
    "        if not self.properties:\n",
    "            print(\"No properties to save!\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(self.properties)\n",
    "        \n",
    "        # Clean description field\n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].astype(str).apply(\n",
    "                lambda x: re.sub(r'<[^>]+>', ' ', x) if x != 'N/A' else x\n",
    "            ).apply(\n",
    "                lambda x: re.sub(r'\\s+', ' ', x).strip() if x != 'N/A' else x\n",
    "            )\n",
    "        \n",
    "        # Remove duplicate properties by URL\n",
    "        df_clean = df.drop_duplicates(subset=['url'], keep='first')\n",
    "        \n",
    "        df_clean.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df_clean)} unique properties to {filename}\")\n",
    "        \n",
    "        if len(df_clean) > 0:\n",
    "            print(f\"\\nData Summary:\")\n",
    "            print(f\"Properties with prices: {len(df_clean[df_clean['price'] != 'N/A'])}\")\n",
    "            print(f\"Properties with addresses: {len(df_clean[df_clean['address'] != 'N/A'])}\")\n",
    "            print(f\"Properties with images: {len(df_clean[df_clean['image_count'] != 0])}\")\n",
    "        \n",
    "        return df_clean\n",
    "\n",
    "async def run_complete_scraper():\n",
    "    scraper = RightmoveRentalScraper()\n",
    "    \n",
    "    # Complete London Boroughs list\n",
    "    london_boroughs = [\n",
    "        # Inner London Boroughs\n",
    "        {\"name\": \"City of London\", \"id\": \"REGION%5E61224\"},\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # More granular price ranges to reduce pages per search\n",
    "    price_ranges = [\n",
    "        \n",
    "        \n",
    "        # Very granular lower price ranges (£10k increments)\n",
    "        \"&minPrice=200000&maxPrice=300000\",\n",
    "        \"&minPrice=300000&maxPrice=400000\",\n",
    "        \"&minPrice=400000&maxPrice=500000\",\n",
    "        \"&minPrice=500000&maxPrice=600000\",\n",
    "        \"&minPrice=600000&maxPrice=700000\",\n",
    "        \"&minPrice=700000&maxPrice=800000\",\n",
    "        \"&minPrice=800000&maxPrice=900000\",\n",
    "        \"&minPrice=900000&maxPrice=1000000\",\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # £100k increments for higher range\n",
    "        \"&minPrice=1000000&maxPrice=1100000\",\n",
    "        \"&minPrice=1100000&maxPrice=1200000\",\n",
    "        \"&minPrice=1200000&maxPrice=1300000\",\n",
    "        \"&minPrice=1300000&maxPrice=1400000\",\n",
    "        \"&minPrice=1400000&maxPrice=1500000\",\n",
    "        \"&minPrice=1500000&maxPrice=1600000\",\n",
    "        \"&minPrice=1600000&maxPrice=1700000\",\n",
    "        \"&minPrice=1700000&maxPrice=1800000\",\n",
    "        \"&minPrice=1800000&maxPrice=1900000\",\n",
    "        \"&minPrice=1900000&maxPrice=2000000\",\n",
    "        \n",
    "        # £250k increments for high range\n",
    "        \"&minPrice=2000000&maxPrice=2250000\",\n",
    "        \"&minPrice=2250000&maxPrice=2500000\",\n",
    "        \"&minPrice=2500000&maxPrice=2750000\",\n",
    "        \"&minPrice=2750000&maxPrice=3000000\",\n",
    "        \"&minPrice=3000000&maxPrice=3500000\",\n",
    "        \"&minPrice=3500000&maxPrice=4000000\",\n",
    "        \"&minPrice=4000000&maxPrice=4500000\",\n",
    "        \"&minPrice=4500000&maxPrice=5000000\",\n",
    "        \n",
    "        # Larger increments for ultra-high end\n",
    "        \"&minPrice=5000000&maxPrice=6000000\",\n",
    "        \"&minPrice=6000000&maxPrice=7000000\",\n",
    "        \"&minPrice=7000000&maxPrice=8000000\",\n",
    "        \"&minPrice=8000000&maxPrice=9000000\",\n",
    "        \"&minPrice=9000000&maxPrice=10000000\",\n",
    "        \"&minPrice=10000000&maxPrice=12000000\",\n",
    "        \"&minPrice=12000000&maxPrice=15000000\",\n",
    "        \"&minPrice=15000000&maxPrice=20000000\",\n",
    "        \n",
    "        # Ultra-high end (no maximum)\n",
    "        \"&minPrice=20000000\"\n",
    "    ]\n",
    "    \n",
    "    # Generate all search URL combinations with proper format\n",
    "    london_search_urls = []\n",
    "    \n",
    "    for borough in london_boroughs:\n",
    "        for price_range in price_ranges:\n",
    "            # Sale properties with proper URL format\n",
    "            sale_url = f\"https://www.rightmove.co.uk/property-for-sale/find.html?useLocationIdentifier=true&locationIdentifier={borough['id']}&buy=For+sale&radius=0.0&_includeSSTC=on&index=0&sortType=2&channel=BUY&transactionType=BUY{price_range}\"\n",
    "            london_search_urls.append({\n",
    "                'url': sale_url,\n",
    "                'name': borough['name'],\n",
    "                'type': 'sale',\n",
    "                'price_range': price_range.replace('&', '') if price_range else 'all_prices'\n",
    "            })\n",
    "    \n",
    "    print(f\"Generated {len(london_search_urls)} London search combinations\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    target_properties = 50000\n",
    "    scraped_urls = set()  # Track to avoid duplicates\n",
    "    \n",
    "    for i, search_item in enumerate(london_search_urls):\n",
    "        search_url = search_item['url']\n",
    "        borough_name = search_item['name']\n",
    "        search_type = search_item['type']\n",
    "        price_range = search_item['price_range']\n",
    "        \n",
    "        print(f\"\\n🏙️ Scraping {borough_name} ({search_type}) - {price_range}\")\n",
    "        print(f\"Current total: {len(scraper.properties)} properties\")\n",
    "        \n",
    "        # Calculate how many more properties we need\n",
    "        remaining_needed = target_properties - len(scraper.properties)\n",
    "        if remaining_needed <= 0:\n",
    "            print(f\"✅ Target of {target_properties} properties reached!\")\n",
    "            break\n",
    "        \n",
    "        # Set max_properties for this search (limit per search combination)\n",
    "        max_for_this_search = min(remaining_needed, 300)  # Reduced from 500 since we have more granular searches\n",
    "        \n",
    "        try:\n",
    "            # Smart page detection - will stop early if no results\n",
    "            await scraper.scrape_all_properties_fast(\n",
    "                search_url, \n",
    "                max_pages=42,  # Max possible, but will stop early\n",
    "                max_properties=max_for_this_search\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {borough_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"✅ {borough_name} ({price_range}) completed. Total properties: {len(scraper.properties)}\")\n",
    "        \n",
    "        # Break if we've reached our target\n",
    "        if len(scraper.properties) >= target_properties:\n",
    "            print(f\"🎯 Target reached! Total: {len(scraper.properties)} properties\")\n",
    "            break\n",
    "        \n",
    "        # Small delay between searches\n",
    "        await asyncio.sleep(0.5)  # Reduced delay\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Trim to exactly 50,000 if we got more\n",
    "    if len(scraper.properties) > target_properties:\n",
    "        scraper.properties = scraper.properties[:target_properties]\n",
    "        print(f\"✂️ Trimmed to exactly {target_properties} properties\")\n",
    "    \n",
    "    # Remove any duplicate properties by URL\n",
    "    seen_urls = set()\n",
    "    unique_properties = []\n",
    "    for prop in scraper.properties:\n",
    "        if prop.get('url') not in seen_urls:\n",
    "            seen_urls.add(prop.get('url'))\n",
    "            unique_properties.append(prop)\n",
    "    \n",
    "    scraper.properties = unique_properties\n",
    "    \n",
    "    scraper.save_to_csv('rightmove_london_50k_comprehensive.csv')\n",
    "    await scraper.httpx_client.aclose()\n",
    "    \n",
    "    print(f\"⚡ Completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"📊 Final count: {len(scraper.properties)} unique London properties\")\n",
    "    print(f\"🏘️ Covered all {len(london_boroughs)} London boroughs/areas\")\n",
    "    return scraper.properties\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    results = asyncio.run(run_complete_scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ec8177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 297 London search combinations\n",
      "\n",
      "🏙️ Scraping City of London (sale) - maxPrice=300000\n",
      "Current total: 0 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 25 properties found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-4342' coro=<run_complete_scraper() done, defined at C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3133418577.py:474> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3133418577.py\", line 623, in <module>\n",
      "    results = asyncio.run(run_complete_scraper())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3133418577.py\", line 576, in run_complete_scraper\n",
      "    await scraper.scrape_all_properties_fast(\n",
      "  File \"C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3133418577.py\", line 412, in scrape_all_properties_fast\n",
      "    property_links = self.get_property_links(search_url, max_pages)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jc\\AppData\\Local\\Temp\\ipykernel_5792\\3133418577.py\", line 130, in get_property_links\n",
      "    response = self.session.get(page_url, timeout=15)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\adapters.py\", line 589, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py\", line 1428, in getresponse\n",
      "    response.begin()\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\socket.py\", line 708, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jc\\anaconda3\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Total unique property links found: 28\n",
      "Found 28 unique properties to scrape...\n",
      "New properties to scrape: 28\n",
      "Completed 3/28 properties (Success: 3)\n",
      "Completed 6/28 properties (Success: 6)\n",
      "Completed 9/28 properties (Success: 9)\n",
      "Completed 12/28 properties (Success: 12)\n",
      "Completed 15/28 properties (Success: 15)\n",
      "Completed 18/28 properties (Success: 18)\n",
      "Completed 21/28 properties (Success: 21)\n",
      "Completed 24/28 properties (Success: 24)\n",
      "Completed 27/28 properties (Success: 27)\n",
      "Completed 28/28 properties (Success: 28)\n",
      "✅ City of London (maxPrice=300000) completed. Total properties: 28\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=300000maxPrice=500000\n",
      "Current total: 28 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Total unique property links found: 45\n",
      "Found 45 unique properties to scrape...\n",
      "New properties to scrape: 44\n",
      "Completed 3/44 properties (Success: 3)\n",
      "Completed 6/44 properties (Success: 6)\n",
      "Completed 9/44 properties (Success: 9)\n",
      "Completed 12/44 properties (Success: 12)\n",
      "Completed 15/44 properties (Success: 15)\n",
      "Completed 18/44 properties (Success: 18)\n",
      "Completed 21/44 properties (Success: 21)\n",
      "Completed 24/44 properties (Success: 24)\n",
      "Completed 27/44 properties (Success: 27)\n",
      "Completed 30/44 properties (Success: 30)\n",
      "Completed 33/44 properties (Success: 33)\n",
      "Completed 36/44 properties (Success: 36)\n",
      "Completed 39/44 properties (Success: 39)\n",
      "Completed 42/44 properties (Success: 42)\n",
      "Completed 44/44 properties (Success: 44)\n",
      "✅ City of London (minPrice=300000maxPrice=500000) completed. Total properties: 72\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=500000maxPrice=750000\n",
      "Current total: 72 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Total unique property links found: 54\n",
      "Found 54 unique properties to scrape...\n",
      "New properties to scrape: 51\n",
      "Completed 3/51 properties (Success: 3)\n",
      "Completed 6/51 properties (Success: 6)\n",
      "Completed 9/51 properties (Success: 9)\n",
      "Completed 12/51 properties (Success: 12)\n",
      "Completed 15/51 properties (Success: 15)\n",
      "Completed 18/51 properties (Success: 18)\n",
      "Completed 21/51 properties (Success: 21)\n",
      "Completed 24/51 properties (Success: 24)\n",
      "Completed 27/51 properties (Success: 27)\n",
      "Completed 30/51 properties (Success: 30)\n",
      "Completed 33/51 properties (Success: 33)\n",
      "Completed 36/51 properties (Success: 36)\n",
      "Completed 39/51 properties (Success: 39)\n",
      "Completed 42/51 properties (Success: 42)\n",
      "Completed 45/51 properties (Success: 45)\n",
      "Completed 48/51 properties (Success: 48)\n",
      "Completed 51/51 properties (Success: 51)\n",
      "✅ City of London (minPrice=500000maxPrice=750000) completed. Total properties: 123\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=750000maxPrice=1000000\n",
      "Current total: 123 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Total unique property links found: 49\n",
      "Found 49 unique properties to scrape...\n",
      "New properties to scrape: 48\n",
      "Completed 3/48 properties (Success: 3)\n",
      "Completed 6/48 properties (Success: 6)\n",
      "Completed 9/48 properties (Success: 9)\n",
      "Completed 12/48 properties (Success: 12)\n",
      "Completed 15/48 properties (Success: 15)\n",
      "Completed 18/48 properties (Success: 18)\n",
      "Completed 21/48 properties (Success: 21)\n",
      "Completed 24/48 properties (Success: 24)\n",
      "Completed 27/48 properties (Success: 27)\n",
      "Completed 30/48 properties (Success: 30)\n",
      "Completed 33/48 properties (Success: 33)\n",
      "Completed 36/48 properties (Success: 36)\n",
      "Completed 39/48 properties (Success: 39)\n",
      "Completed 42/48 properties (Success: 42)\n",
      "Completed 45/48 properties (Success: 45)\n",
      "Completed 48/48 properties (Success: 48)\n",
      "✅ City of London (minPrice=750000maxPrice=1000000) completed. Total properties: 171\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=1000000maxPrice=1500000\n",
      "Current total: 171 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Total unique property links found: 55\n",
      "Found 55 unique properties to scrape...\n",
      "New properties to scrape: 55\n",
      "Completed 3/55 properties (Success: 3)\n",
      "Completed 6/55 properties (Success: 6)\n",
      "Completed 9/55 properties (Success: 9)\n",
      "Completed 12/55 properties (Success: 12)\n",
      "Completed 15/55 properties (Success: 15)\n",
      "Completed 18/55 properties (Success: 18)\n",
      "Completed 21/55 properties (Success: 21)\n",
      "Completed 24/55 properties (Success: 24)\n",
      "Completed 27/55 properties (Success: 27)\n",
      "Completed 30/55 properties (Success: 30)\n",
      "Completed 33/55 properties (Success: 33)\n",
      "Completed 36/55 properties (Success: 36)\n",
      "Completed 39/55 properties (Success: 39)\n",
      "Completed 42/55 properties (Success: 42)\n",
      "Completed 45/55 properties (Success: 45)\n",
      "Completed 48/55 properties (Success: 48)\n",
      "Completed 51/55 properties (Success: 51)\n",
      "Completed 54/55 properties (Success: 54)\n",
      "Completed 55/55 properties (Success: 55)\n",
      "✅ City of London (minPrice=1000000maxPrice=1500000) completed. Total properties: 226\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=1500000maxPrice=2000000\n",
      "Current total: 226 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n",
      "Page 39: 25 properties found\n",
      "Page 40: 25 properties found\n",
      "Page 41: 25 properties found\n",
      "Page 42: 25 properties found\n",
      "Total unique property links found: 45\n",
      "Found 45 unique properties to scrape...\n",
      "New properties to scrape: 42\n",
      "Completed 3/42 properties (Success: 3)\n",
      "Completed 6/42 properties (Success: 6)\n",
      "Completed 9/42 properties (Success: 9)\n",
      "Completed 12/42 properties (Success: 12)\n",
      "Completed 15/42 properties (Success: 15)\n",
      "Completed 18/42 properties (Success: 18)\n",
      "Completed 21/42 properties (Success: 21)\n",
      "Completed 24/42 properties (Success: 24)\n",
      "Completed 27/42 properties (Success: 27)\n",
      "Completed 30/42 properties (Success: 30)\n",
      "Completed 33/42 properties (Success: 33)\n",
      "Completed 36/42 properties (Success: 36)\n",
      "Completed 39/42 properties (Success: 39)\n",
      "Completed 42/42 properties (Success: 42)\n",
      "✅ City of London (minPrice=1500000maxPrice=2000000) completed. Total properties: 268\n",
      "\n",
      "🏙️ Scraping City of London (sale) - minPrice=2000000maxPrice=3000000\n",
      "Current total: 268 properties\n",
      "Getting property links (will stop early if no results found)...\n",
      "Page 1: 25 properties found\n",
      "Page 2: 25 properties found\n",
      "Page 3: 25 properties found\n",
      "Page 4: 25 properties found\n",
      "Page 5: 25 properties found\n",
      "Page 6: 25 properties found\n",
      "Page 7: 25 properties found\n",
      "Page 8: 25 properties found\n",
      "Page 9: 25 properties found\n",
      "Page 10: 25 properties found\n",
      "Page 11: 25 properties found\n",
      "Page 12: 25 properties found\n",
      "Page 13: 25 properties found\n",
      "Page 14: 25 properties found\n",
      "Page 15: 25 properties found\n",
      "Page 16: 25 properties found\n",
      "Page 17: 25 properties found\n",
      "Page 18: 25 properties found\n",
      "Page 19: 25 properties found\n",
      "Page 20: 25 properties found\n",
      "Page 21: 25 properties found\n",
      "Page 22: 25 properties found\n",
      "Page 23: 25 properties found\n",
      "Page 24: 25 properties found\n",
      "Page 25: 25 properties found\n",
      "Page 26: 25 properties found\n",
      "Page 27: 25 properties found\n",
      "Page 28: 25 properties found\n",
      "Page 29: 25 properties found\n",
      "Page 30: 25 properties found\n",
      "Page 31: 25 properties found\n",
      "Page 32: 25 properties found\n",
      "Page 33: 25 properties found\n",
      "Page 34: 25 properties found\n",
      "Page 35: 25 properties found\n",
      "Page 36: 25 properties found\n",
      "Page 37: 25 properties found\n",
      "Page 38: 25 properties found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 610\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    609\u001b[0m     nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m--> 610\u001b[0m     results \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(run_complete_scraper())\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\events.py:88\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:396\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:303\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    301\u001b[0m _enter_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step_run_and_handle_result(exc)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     _leave_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[11], line 569\u001b[0m, in \u001b[0;36mrun_complete_scraper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    566\u001b[0m max_for_this_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(remaining_needed, \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mscrape_all_properties_fast(\n\u001b[0;32m    570\u001b[0m         search_url, \n\u001b[0;32m    571\u001b[0m         max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,  \u001b[38;5;66;03m# Reduced for testing\u001b[39;00m\n\u001b[0;32m    572\u001b[0m         max_properties\u001b[38;5;241m=\u001b[39mmax_for_this_search\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError scraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborough_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 408\u001b[0m, in \u001b[0;36mRightmoveRentalScraper.scrape_all_properties_fast\u001b[1;34m(self, search_url, max_pages, max_properties)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_all_properties_fast\u001b[39m(\u001b[38;5;28mself\u001b[39m, search_url, max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_properties\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting property links (will stop early if no results found)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 408\u001b[0m     property_links \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_property_links(search_url, max_pages)\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_properties:\n\u001b[0;32m    411\u001b[0m         property_links \u001b[38;5;241m=\u001b[39m property_links[:max_properties]\n",
      "Cell \u001b[1;32mIn[11], line 163\u001b[0m, in \u001b[0;36mRightmoveRentalScraper.get_property_links\u001b[1;34m(self, search_url, max_pages)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(links)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m properties found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Add a small delay to be respectful\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import jmespath\n",
    "import asyncio\n",
    "from httpx import AsyncClient\n",
    "from parsel import Selector\n",
    "import nest_asyncio\n",
    "\n",
    "class RightmoveRentalScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.properties = []\n",
    "        self.scraped_urls = set()  # Track scraped URLs to avoid duplicates\n",
    "        self.httpx_client = AsyncClient(\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            },\n",
    "            follow_redirects=True,\n",
    "            http2=False,\n",
    "            timeout=30,\n",
    "        )\n",
    "    \n",
    "    def find_json_objects(self, text: str):\n",
    "        pos = 0\n",
    "        while True:\n",
    "            match = text.find(\"{\", pos)\n",
    "            if match == -1: break\n",
    "            try:\n",
    "                result, index = json.JSONDecoder().raw_decode(text[match:])\n",
    "                yield result\n",
    "                pos = match + index\n",
    "            except:\n",
    "                pos = match + 1\n",
    "\n",
    "    def extract_property_id(self, url):\n",
    "        \"\"\"Extract property ID from URL\"\"\"\n",
    "        match = re.search(r'/properties/(\\d+)', url)\n",
    "        return match.group(1) if match else 'N/A'\n",
    "    \n",
    "    def format_address_and_postcode(self, address_data):\n",
    "        \"\"\"Extract and format address and postcode from JSON address object\"\"\"\n",
    "        if isinstance(address_data, dict):\n",
    "            # Extract display address and clean it\n",
    "            display_address = address_data.get('displayAddress', '')\n",
    "            if display_address:\n",
    "                # Remove line breaks and extra spaces\n",
    "                clean_address = re.sub(r'\\r\\n|\\r|\\n', ', ', display_address)\n",
    "                clean_address = re.sub(r',\\s*,', ',', clean_address)  # Remove double commas\n",
    "                clean_address = re.sub(r'\\s+', ' ', clean_address).strip()  # Normalize spaces\n",
    "                \n",
    "                # Remove postcode from address if it appears at the end\n",
    "                postcode_pattern = r',?\\s*[A-Z]{1,2}\\d{1,2}[A-Z]?\\s*\\d[A-Z]{2}\\s*$'\n",
    "                clean_address = re.sub(postcode_pattern, '', clean_address, flags=re.IGNORECASE)\n",
    "                clean_address = clean_address.rstrip(', ')\n",
    "            else:\n",
    "                clean_address = 'N/A'\n",
    "            \n",
    "            # Combine outcode and incode to form postcode\n",
    "            outcode = address_data.get('outcode', '')\n",
    "            incode = address_data.get('incode', '')\n",
    "            \n",
    "            if outcode and incode:\n",
    "                postcode = f\"{outcode} {incode}\"\n",
    "            else:\n",
    "                postcode = 'N/A'\n",
    "            \n",
    "            return clean_address, postcode\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    def extract_tenure_details(self, json_data):\n",
    "        \"\"\"Extract tenure type and lease years remaining from JSON data\"\"\"\n",
    "        tenure_type = 'N/A'\n",
    "        lease_years_remaining = 'N/A'\n",
    "        \n",
    "        if json_data:\n",
    "            # Extract tenure information\n",
    "            tenure_info = jmespath.search(\"tenure\", json_data)\n",
    "            \n",
    "            if isinstance(tenure_info, dict):\n",
    "                # Extract tenure type\n",
    "                tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                \n",
    "                # Extract years remaining on lease\n",
    "                years_remaining = tenure_info.get('yearsRemainingOnLease')\n",
    "                if years_remaining is not None:\n",
    "                    lease_years_remaining = years_remaining\n",
    "                \n",
    "                # Handle message field if needed\n",
    "                message = tenure_info.get('message')\n",
    "                if message:\n",
    "                    # You can append message to tenure_type if it contains useful info\n",
    "                    tenure_type = f\"{tenure_type} - {message}\"\n",
    "            \n",
    "            elif isinstance(tenure_info, str):\n",
    "                # If it's just a string, use it as tenure type\n",
    "                tenure_type = tenure_info\n",
    "        \n",
    "        return tenure_type, lease_years_remaining\n",
    "\n",
    "    async def extract_property_json_data(self, property_url):\n",
    "        try:\n",
    "            response = await self.httpx_client.get(property_url)\n",
    "            data = Selector(response.text).xpath(\"//script[contains(.,'PAGE_MODEL = ')]/text()\").get()\n",
    "            if data:\n",
    "                json_objects = list(self.find_json_objects(data))\n",
    "                for obj in json_objects:\n",
    "                    if \"propertyData\" in obj:\n",
    "                        return obj[\"propertyData\"]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"JSON extraction error for {property_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_property_links(self, search_url, max_pages=42):\n",
    "        \"\"\"Get property links with smart page detection - stops early if no results\"\"\"\n",
    "        property_links = []\n",
    "        consecutive_empty_pages = 0\n",
    "        \n",
    "        for page in range(max_pages):\n",
    "            page_url = f\"{search_url}&index={page * 24}\"\n",
    "            try:\n",
    "                response = self.session.get(page_url, timeout=15)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Try multiple selectors to find property links\n",
    "                links = []\n",
    "                selectors = [\n",
    "                    'a.propertyCard-link',\n",
    "                    'a[href*=\"/properties/\"]',\n",
    "                    '.propertyCard a',\n",
    "                    '[data-test=\"property-card\"] a',\n",
    "                    'a[href*=\"/property-for-sale/\"]'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors:\n",
    "                    found_links = soup.select(selector)\n",
    "                    if found_links:\n",
    "                        links = [f\"https://www.rightmove.co.uk{link.get('href')}\" \n",
    "                                for link in found_links if link.get('href') and '/properties/' in link.get('href')]\n",
    "                        break\n",
    "                \n",
    "                if len(links) == 0:\n",
    "                    consecutive_empty_pages += 1\n",
    "                    print(f\"Page {page + 1}: 0 properties found\")\n",
    "                    # Stop after 2 consecutive empty pages\n",
    "                    if consecutive_empty_pages >= 2:\n",
    "                        print(f\"No more results found after page {page + 1}. Stopping early.\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_pages = 0\n",
    "                    property_links.extend(links)\n",
    "                    print(f\"Page {page + 1}: {len(links)} properties found\")\n",
    "                \n",
    "                # Add a small delay to be respectful\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page + 1}: {e}\")\n",
    "                consecutive_empty_pages += 1\n",
    "                if consecutive_empty_pages >= 2:\n",
    "                    print(\"Too many consecutive errors. Stopping page search.\")\n",
    "                    break\n",
    "                \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_links = []\n",
    "        for link in property_links:\n",
    "            if link not in seen:\n",
    "                seen.add(link)\n",
    "                unique_links.append(link)\n",
    "        \n",
    "        print(f\"Total unique property links found: {len(unique_links)}\")\n",
    "        return unique_links\n",
    "\n",
    "    async def scrape_property_details_fast(self, property_url):\n",
    "        # Skip if already scraped\n",
    "        if property_url in self.scraped_urls:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            html_task = asyncio.create_task(self.httpx_client.get(property_url))\n",
    "            json_data = await self.extract_property_json_data(property_url)\n",
    "            response = await html_task\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            property_data = {'url': property_url}\n",
    "            \n",
    "            # Mark as scraped\n",
    "            self.scraped_urls.add(property_url)\n",
    "            \n",
    "            # Extract property ID and use as title\n",
    "            property_id = self.extract_property_id(property_url)\n",
    "            property_data['title'] = property_id\n",
    "            \n",
    "            # Basic property info with enhanced extraction\n",
    "            all_text = soup.get_text()\n",
    "            \n",
    "            # Enhanced bedroom extraction\n",
    "            bed_patterns = [\n",
    "                r'(\\d+)\\s*bedroom',\n",
    "                r'(\\d+)\\s*bed(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bed'\n",
    "            ]\n",
    "            property_data['bedrooms'] = 'N/A'\n",
    "            for pattern in bed_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bedrooms'] = match.group(1)\n",
    "                    break\n",
    "            \n",
    "            # Enhanced bathroom extraction\n",
    "            bath_patterns = [\n",
    "                r'(\\d+)\\s*bathroom',\n",
    "                r'(\\d+)\\s*bath(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bath'\n",
    "            ]\n",
    "            property_data['bathrooms'] = 'N/A'\n",
    "            for pattern in bath_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bathrooms'] = match.group(1)\n",
    "                    break\n",
    "\n",
    "            # Key features extraction with multiple methods\n",
    "            property_data['key_features'] = 'N/A'\n",
    "            \n",
    "            # Method 1: Look for feature lists\n",
    "            feature_selectors = [\n",
    "                'ul.lIhZ24u1NHlVy5_W9V__6 li',\n",
    "                '.key-features li',\n",
    "                '[data-test=\"key-features\"] li',\n",
    "                'h2:contains(\"Key features\") + ul li',\n",
    "                'h3:contains(\"Key features\") + ul li'\n",
    "            ]\n",
    "            \n",
    "            for selector in feature_selectors:\n",
    "                features = soup.select(selector)\n",
    "                if features:\n",
    "                    feature_texts = [f.get_text().strip() for f in features]\n",
    "                    property_data['key_features'] = '; '.join(feature_texts)\n",
    "                    break\n",
    "            \n",
    "            # Method 2: Look for feature headers and following lists\n",
    "            if property_data['key_features'] == 'N/A':\n",
    "                feature_headers = soup.find_all(['h2', 'h3'], string=re.compile(r'key features|features|amenities', re.I))\n",
    "                for header in feature_headers:\n",
    "                    next_element = header.find_next_sibling()\n",
    "                    while next_element:\n",
    "                        if next_element.name in ['ul', 'ol']:\n",
    "                            features = [li.get_text().strip() for li in next_element.find_all('li')]\n",
    "                            if features:\n",
    "                                property_data['key_features'] = '; '.join(features)\n",
    "                                break\n",
    "                        next_element = next_element.find_next_sibling()\n",
    "                    if property_data['key_features'] != 'N/A':\n",
    "                        break\n",
    "            \n",
    "            # Property details extraction\n",
    "            property_data.update({\n",
    "                'parking': 'N/A',\n",
    "                'garden': 'N/A',\n",
    "                'council_tax': 'N/A',\n",
    "                'accessibility': 'N/A',\n",
    "                'size_sqft': 'N/A',\n",
    "                'size_sqm': 'N/A',\n",
    "                'furnish_status': 'N/A',\n",
    "            })\n",
    "           \n",
    "            # Extract details from various selectors\n",
    "            detail_selectors = [\n",
    "                'dt._17A0LehXZKxGHbPeiLQ1BI',\n",
    "                'dt[class*=\"detail\"]',\n",
    "                '.property-details dt',\n",
    "                'dl dt'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                detail_sections = soup.select(selector)\n",
    "                if detail_sections:\n",
    "                    for section in detail_sections:\n",
    "                        section_text = section.get_text().strip().upper()\n",
    "                        value_element = section.find_next_sibling(['dd', 'span', 'div'])\n",
    "                        value_text = value_element.get_text().strip() if value_element else 'N/A'\n",
    "                        \n",
    "                        if 'PARKING' in section_text:\n",
    "                            property_data['parking'] = value_text\n",
    "                        elif 'GARDEN' in section_text:\n",
    "                            property_data['garden'] = value_text\n",
    "                        elif 'COUNCIL TAX' in section_text:\n",
    "                            property_data['council_tax'] = value_text\n",
    "                        elif 'ACCESSIBILITY' in section_text:\n",
    "                            property_data['accessibility'] = value_text\n",
    "                        elif 'FURNISH' in section_text:\n",
    "                            property_data['furnish_status'] = value_text\n",
    "                    break\n",
    "\n",
    "            # Additional furnish status extraction\n",
    "            furnish_dt = soup.find('dt', string=re.compile(r'^Furnish type:\\s*$', re.I))\n",
    "            if furnish_dt:\n",
    "                furnish_dd = furnish_dt.find_next_sibling('dd')\n",
    "                if furnish_dd:\n",
    "                    property_data['furnish_status'] = furnish_dd.get_text().strip()\n",
    "\n",
    "            # Size extraction from full text\n",
    "            sqft_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*ft',\n",
    "                r'([0-9,]+)\\s*sqft',\n",
    "                r'([0-9,]+)\\s*square\\s*feet'\n",
    "            ]\n",
    "            for pattern in sqft_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqft'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "            \n",
    "            sqm_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*m',\n",
    "                r'([0-9,]+)\\s*sqm',\n",
    "                r'([0-9,]+)\\s*square\\s*metres?'\n",
    "            ]\n",
    "            for pattern in sqm_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqm'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "\n",
    "            # Extract data from JSON if available\n",
    "            if json_data:\n",
    "                try:\n",
    "                    stations = jmespath.search(\"nearestStations[*].{name: name, distance: distance}\", json_data) or []\n",
    "                    photos = jmespath.search(\"images[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    floorplans = jmespath.search(\"floorplans[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    property_type = (jmespath.search(\"propertySubType\", json_data) or \n",
    "                                   jmespath.search(\"propertyType\", json_data) or 'N/A')\n",
    "                    description = jmespath.search(\"text.description\", json_data) or 'N/A'\n",
    "                    address_data = jmespath.search(\"address\", json_data)\n",
    "                    latitude = jmespath.search(\"location.latitude\", json_data) or 'N/A'\n",
    "                    longitude = jmespath.search(\"location.longitude\", json_data) or 'N/A'\n",
    "                    price = jmespath.search(\"prices.primaryPrice\", json_data) or 'N/A'\n",
    "                    \n",
    "                    # Tenure information using the dedicated method\n",
    "                    tenure_type, lease_years_remaining = self.extract_tenure_details(json_data)\n",
    "                    \n",
    "                    formatted_address, postcode = self.format_address_and_postcode(address_data)\n",
    "                    \n",
    "                    property_data.update({\n",
    "                        'nearest_stations': '; '.join([s['name'] for s in stations]) or 'N/A',\n",
    "                        'station_distances': '; '.join([f\"{s['distance']} miles\" for s in stations]) or 'N/A',\n",
    "                        'station_count': len(stations),\n",
    "                        'image_count': len(photos),\n",
    "                        'image_urls': '; '.join([p['url'] for p in photos]) or 'N/A',\n",
    "                        'floorplan_count': len(floorplans),\n",
    "                        'floorplan_urls': '; '.join([f['url'] for f in floorplans]) or 'N/A',\n",
    "                        'property_type': property_type,\n",
    "                        'tenure_type': tenure_type,\n",
    "                        'lease_years_remaining': lease_years_remaining,\n",
    "                        'description': description,\n",
    "                        'latitude': latitude,\n",
    "                        'longitude': longitude,\n",
    "                        'address': formatted_address,\n",
    "                        'postcode': postcode,\n",
    "                        'price': price\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting JSON data: {e}\")\n",
    "            else:\n",
    "                # HTML fallback for property type if JSON fails\n",
    "                property_type_fallback = 'N/A'\n",
    "                type_element = soup.find(string=re.compile(r'(flat|house|apartment|studio|maisonette)', re.I))\n",
    "                if type_element:\n",
    "                    property_type_fallback = type_element.strip()\n",
    "                \n",
    "                property_data.update({\n",
    "                    'nearest_stations': 'N/A',\n",
    "                    'station_distances': 'N/A', \n",
    "                    'station_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'image_urls': 'N/A',\n",
    "                    'floorplan_count': 0,\n",
    "                    'floorplan_urls': 'N/A',\n",
    "                    'property_type': property_type_fallback,\n",
    "                    'description': 'N/A',\n",
    "                    'tenure_type': 'N/A',\n",
    "                    'lease_years_remaining': 'N/A',\n",
    "                    'latitude': 'N/A',\n",
    "                    'longitude': 'N/A',\n",
    "                    'address': 'N/A',\n",
    "                    'postcode': 'N/A',\n",
    "                    'price': 'N/A'\n",
    "                })\n",
    "            \n",
    "            return property_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {property_url}: {e}\")\n",
    "            return {'url': property_url, 'error': str(e)}\n",
    "\n",
    "    async def scrape_all_properties_fast(self, search_url, max_pages=42, max_properties=None):\n",
    "        print(f\"Getting property links (will stop early if no results found)...\")\n",
    "        property_links = self.get_property_links(search_url, max_pages)\n",
    "        \n",
    "        if max_properties:\n",
    "            property_links = property_links[:max_properties]\n",
    "        \n",
    "        print(f\"Found {len(property_links)} unique properties to scrape...\")\n",
    "        \n",
    "        if not property_links:\n",
    "            print(\"No property links found.\")\n",
    "            return\n",
    "        \n",
    "        # Filter out already scraped properties\n",
    "        new_links = [link for link in property_links if link not in self.scraped_urls]\n",
    "        print(f\"New properties to scrape: {len(new_links)}\")\n",
    "        \n",
    "        if not new_links:\n",
    "            print(\"All properties already scraped.\")\n",
    "            return\n",
    "        \n",
    "        batch_size = 3  # Conservative batch size\n",
    "        successful = 0\n",
    "        \n",
    "        for i in range(0, len(new_links), batch_size):\n",
    "            batch = new_links[i:i+batch_size]\n",
    "            tasks = [self.scrape_property_details_fast(url) for url in batch]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            for result in results:\n",
    "                if isinstance(result, dict) and result is not None and 'error' not in result:\n",
    "                    self.properties.append(result)\n",
    "                    successful += 1\n",
    "                elif isinstance(result, dict) and 'error' in result:\n",
    "                    print(f\"Error in result: {result['error']}\")\n",
    "            \n",
    "            print(f\"Completed {min(i+batch_size, len(new_links))}/{len(new_links)} properties (Success: {successful})\")\n",
    "            await asyncio.sleep(0.5)  # Reduced delay\n",
    "\n",
    "    def save_to_csv(self, filename='rightmove_london_properties.csv'):\n",
    "        if not self.properties:\n",
    "            print(\"No properties to save!\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(self.properties)\n",
    "        \n",
    "        # Clean description field\n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].astype(str).apply(\n",
    "                lambda x: re.sub(r'<[^>]+>', ' ', x) if x != 'N/A' else x\n",
    "            ).apply(\n",
    "                lambda x: re.sub(r'\\s+', ' ', x).strip() if x != 'N/A' else x\n",
    "            )\n",
    "        \n",
    "        # Remove duplicate properties by URL\n",
    "        df_clean = df.drop_duplicates(subset=['url'], keep='first')\n",
    "        \n",
    "        df_clean.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df_clean)} unique properties to {filename}\")\n",
    "        \n",
    "        if len(df_clean) > 0:\n",
    "            print(f\"\\nData Summary:\")\n",
    "            print(f\"Properties with prices: {len(df_clean[df_clean['price'] != 'N/A'])}\")\n",
    "            print(f\"Properties with addresses: {len(df_clean[df_clean['address'] != 'N/A'])}\")\n",
    "            print(f\"Properties with images: {len(df_clean[df_clean['image_count'] != 0])}\")\n",
    "        \n",
    "        return df_clean\n",
    "\n",
    "async def run_complete_scraper():\n",
    "    scraper = RightmoveRentalScraper()\n",
    "    \n",
    "    # Complete London Boroughs list\n",
    "    london_boroughs = [\n",
    "        # Inner London Boroughs\n",
    "        {\"name\": \"City of London\", \"id\": \"REGION%5E92824\"},\n",
    "        {\"name\": \"Camden\", \"id\": \"REGION%5E1465\"},\n",
    "        {\"name\": \"Greenwich\", \"id\": \"REGION%5E1467\"},\n",
    "        {\"name\": \"Hackney\", \"id\": \"REGION%5E1468\"},\n",
    "        {\"name\": \"Hammersmith and Fulham\", \"id\": \"REGION%5E1469\"},\n",
    "        {\"name\": \"Islington\", \"id\": \"REGION%5E1470\"},\n",
    "        {\"name\": \"Kensington and Chelsea\", \"id\": \"REGION%5E1471\"},\n",
    "        {\"name\": \"Lambeth\", \"id\": \"REGION%5E1472\"},\n",
    "        {\"name\": \"Lewisham\", \"id\": \"REGION%5E1473\"},\n",
    "        {\"name\": \"Southwark\", \"id\": \"REGION%5E1474\"},\n",
    "        {\"name\": \"Tower Hamlets\", \"id\": \"REGION%5E1475\"},\n",
    "        {\"name\": \"Wandsworth\", \"id\": \"REGION%5E1476\"},\n",
    "        {\"name\": \"Westminster\", \"id\": \"REGION%5E1477\"},\n",
    "        \n",
    "        # Outer London Boroughs\n",
    "        {\"name\": \"Barking and Dagenham\", \"id\": \"REGION%5E1478\"},\n",
    "        {\"name\": \"Barnet\", \"id\": \"REGION%5E1479\"},\n",
    "        {\"name\": \"Bexley\", \"id\": \"REGION%5E1480\"},\n",
    "        {\"name\": \"Brent\", \"id\": \"REGION%5E1481\"},\n",
    "        {\"name\": \"Bromley\", \"id\": \"REGION%5E1482\"},\n",
    "        {\"name\": \"Croydon\", \"id\": \"REGION%5E1483\"},\n",
    "        {\"name\": \"Ealing\", \"id\": \"REGION%5E1484\"},\n",
    "        {\"name\": \"Enfield\", \"id\": \"REGION%5E1485\"},\n",
    "        {\"name\": \"Haringey\", \"id\": \"REGION%5E1486\"},\n",
    "        {\"name\": \"Harrow\", \"id\": \"REGION%5E1487\"},\n",
    "        {\"name\": \"Havering\", \"id\": \"REGION%5E1488\"},\n",
    "        {\"name\": \"Hillingdon\", \"id\": \"REGION%5E1489\"},\n",
    "        {\"name\": \"Hounslow\", \"id\": \"REGION%5E1490\"},\n",
    "        {\"name\": \"Kingston upon Thames\", \"id\": \"REGION%5E1491\"},\n",
    "        {\"name\": \"Merton\", \"id\": \"REGION%5E1492\"},\n",
    "        {\"name\": \"Newham\", \"id\": \"REGION%5E1493\"},\n",
    "        {\"name\": \"Redbridge\", \"id\": \"REGION%5E1494\"},\n",
    "        {\"name\": \"Richmond upon Thames\", \"id\": \"REGION%5E1495\"},\n",
    "        {\"name\": \"Sutton\", \"id\": \"REGION%5E1496\"},\n",
    "        {\"name\": \"Waltham Forest\", \"id\": \"REGION%5E1497\"}\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # More granular price ranges to reduce pages per search\n",
    "    price_ranges = [\n",
    "        # Start with broader ranges, then get more granular\n",
    "        \"&maxPrice=300000\",\n",
    "        \"&minPrice=300000&maxPrice=500000\",\n",
    "        \"&minPrice=500000&maxPrice=750000\",\n",
    "        \"&minPrice=750000&maxPrice=1000000\",\n",
    "        \"&minPrice=1000000&maxPrice=1500000\",\n",
    "        \"&minPrice=1500000&maxPrice=2000000\",\n",
    "        \"&minPrice=2000000&maxPrice=3000000\",\n",
    "        \"&minPrice=3000000&maxPrice=5000000\",\n",
    "        \"&minPrice=5000000\"\n",
    "    ]\n",
    "    \n",
    "    # Generate search URLs\n",
    "    london_search_urls = []\n",
    "    \n",
    "    for borough in london_boroughs:\n",
    "        for price_range in price_ranges:\n",
    "            sale_url = f\"https://www.rightmove.co.uk/property-for-sale/find.html?useLocationIdentifier=true&locationIdentifier={borough['id']}&buy=For+sale&radius=0.0&_includeSSTC=on&index=0&sortType=2&channel=BUY&transactionType=BUY{price_range}\"\n",
    "            london_search_urls.append({\n",
    "                'url': sale_url,\n",
    "                'name': borough['name'],\n",
    "                'type': 'sale',\n",
    "                'price_range': price_range.replace('&', '') if price_range else 'all_prices'\n",
    "            })\n",
    "    \n",
    "    print(f\"Generated {len(london_search_urls)} London search combinations\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    target_properties = 50000  # Reduced target for testing\n",
    "    \n",
    "    for i, search_item in enumerate(london_search_urls):\n",
    "        search_url = search_item['url']\n",
    "        borough_name = search_item['name']\n",
    "        search_type = search_item['type']\n",
    "        price_range = search_item['price_range']\n",
    "        \n",
    "        print(f\"\\n🏙️ Scraping {borough_name} ({search_type}) - {price_range}\")\n",
    "        print(f\"Current total: {len(scraper.properties)} properties\")\n",
    "        \n",
    "        # Calculate how many more properties we need\n",
    "        remaining_needed = target_properties - len(scraper.properties)\n",
    "        if remaining_needed <= 0:\n",
    "            print(f\"✅ Target of {target_properties} properties reached!\")\n",
    "            break\n",
    "        \n",
    "        # Set max_properties for this search\n",
    "        max_for_this_search = min(remaining_needed, 200)\n",
    "        \n",
    "        try:\n",
    "            await scraper.scrape_all_properties_fast(\n",
    "                search_url, \n",
    "                max_pages=42,  # Reduced for testing\n",
    "                max_properties=max_for_this_search\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {borough_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"✅ {borough_name} ({price_range}) completed. Total properties: {len(scraper.properties)}\")\n",
    "        \n",
    "        # Break if we've reached our target\n",
    "        if len(scraper.properties) >= target_properties:\n",
    "            print(f\"🎯 Target reached! Total: {len(scraper.properties)} properties\")\n",
    "            break\n",
    "        \n",
    "        # Small delay between searches\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Remove any duplicate properties by URL\n",
    "    seen_urls = set()\n",
    "    unique_properties = []\n",
    "    for prop in scraper.properties:\n",
    "        if prop.get('url') not in seen_urls:\n",
    "            seen_urls.add(prop.get('url'))\n",
    "            unique_properties.append(prop)\n",
    "    \n",
    "    scraper.properties = unique_properties\n",
    "    \n",
    "    scraper.save_to_csv('rightmove_london_properties.csv')\n",
    "    await scraper.httpx_client.aclose()\n",
    "    \n",
    "    print(f\"⚡ Completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"📊 Final count: {len(scraper.properties)} unique London properties\")\n",
    "    return scraper.properties\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    results = asyncio.run(run_complete_scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c47381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏙️ Scraping City of London - maxPrice=400000\n",
      "Current total: 0 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 15 properties found, 15 new, 0 duplicates\n",
      "Page 2: 15 properties found, 0 new, 15 duplicates\n",
      "Page 3: 15 properties found, 0 new, 15 duplicates\n",
      "Page 4: 15 properties found, 0 new, 15 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 14 new properties. Total: 14\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=400000maxPrice=600000\n",
      "Current total: 14 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 25 properties found, 25 new, 0 duplicates\n",
      "Page 2: 25 properties found, 0 new, 25 duplicates\n",
      "Page 3: 25 properties found, 0 new, 25 duplicates\n",
      "Page 4: 25 properties found, 0 new, 25 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 24 new properties. Total: 38\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=600000maxPrice=800000\n",
      "Current total: 38 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 25 properties found, 25 new, 0 duplicates\n",
      "Page 2: 25 properties found, 0 new, 25 duplicates\n",
      "Page 3: 25 properties found, 0 new, 25 duplicates\n",
      "Page 4: 25 properties found, 0 new, 25 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 25 new properties. Total: 63\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=800000maxPrice=1000000\n",
      "Current total: 63 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 25 properties found, 25 new, 0 duplicates\n",
      "Page 2: 25 properties found, 0 new, 25 duplicates\n",
      "Page 3: 25 properties found, 0 new, 25 duplicates\n",
      "Page 4: 25 properties found, 0 new, 25 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 24 new properties. Total: 87\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=1000000maxPrice=1500000\n",
      "Current total: 87 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 25 properties found, 25 new, 0 duplicates\n",
      "Page 2: 25 properties found, 1 new, 24 duplicates\n",
      "Page 3: 25 properties found, 1 new, 24 duplicates\n",
      "Page 4: 25 properties found, 0 new, 25 duplicates\n",
      "Page 5: 25 properties found, 0 new, 25 duplicates\n",
      "Page 6: 25 properties found, 0 new, 25 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 26 new properties. Total: 113\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=1500000maxPrice=2000000\n",
      "Current total: 113 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 24 properties found, 23 new, 1 duplicates\n",
      "Page 2: 24 properties found, 0 new, 24 duplicates\n",
      "Page 3: 24 properties found, 0 new, 24 duplicates\n",
      "Page 4: 24 properties found, 0 new, 24 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 23 new properties. Total: 136\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=2000000maxPrice=3000000\n",
      "Current total: 136 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 18 properties found, 15 new, 3 duplicates\n",
      "Page 2: 18 properties found, 0 new, 18 duplicates\n",
      "Page 3: 18 properties found, 0 new, 18 duplicates\n",
      "Page 4: 18 properties found, 0 new, 18 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 15 new properties. Total: 151\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=3000000\n",
      "Current total: 151 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 19 properties found, 18 new, 1 duplicates\n",
      "Page 2: 19 properties found, 0 new, 19 duplicates\n",
      "Page 3: 19 properties found, 0 new, 19 duplicates\n",
      "Page 4: 19 properties found, 0 new, 19 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 18 new properties. Total: 169\n",
      "\n",
      "🏙️ Scraping Camden - maxPrice=400000\n",
      "Current total: 169 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 25 properties found, 25 new, 0 duplicates\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 539\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m--> 539\u001b[0m     results \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(run_complete_scraper())\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\windows_events.py:445\u001b[0m, in \u001b[0;36mIocpProactor.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results:\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n\u001b[0;32m    446\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\windows_events.py:774\u001b[0m, in \u001b[0;36mIocpProactor._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout too big\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     status \u001b[38;5;241m=\u001b[39m _overlapped\u001b[38;5;241m.\u001b[39mGetQueuedCompletionStatus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iocp, ms)\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    776\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import jmespath\n",
    "import asyncio\n",
    "from httpx import AsyncClient\n",
    "from parsel import Selector\n",
    "import nest_asyncio\n",
    "\n",
    "class RightmoveRentalScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.properties = []\n",
    "        self.scraped_urls = set()  # Track scraped URLs to avoid duplicates\n",
    "        self.httpx_client = AsyncClient(\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            },\n",
    "            follow_redirects=True,\n",
    "            http2=False,\n",
    "            timeout=30,\n",
    "        )\n",
    "    \n",
    "    def find_json_objects(self, text: str):\n",
    "        pos = 0\n",
    "        while True:\n",
    "            match = text.find(\"{\", pos)\n",
    "            if match == -1: break\n",
    "            try:\n",
    "                result, index = json.JSONDecoder().raw_decode(text[match:])\n",
    "                yield result\n",
    "                pos = match + index\n",
    "            except:\n",
    "                pos = match + 1\n",
    "\n",
    "    def extract_property_id(self, url):\n",
    "        \"\"\"Extract property ID from URL\"\"\"\n",
    "        match = re.search(r'/properties/(\\d+)', url)\n",
    "        return match.group(1) if match else 'N/A'\n",
    "    \n",
    "    def format_address_and_postcode(self, address_data):\n",
    "        \"\"\"Extract and format address and postcode from JSON address object\"\"\"\n",
    "        if isinstance(address_data, dict):\n",
    "            display_address = address_data.get('displayAddress', '')\n",
    "            if display_address:\n",
    "                clean_address = re.sub(r'\\r\\n|\\r|\\n', ', ', display_address)\n",
    "                clean_address = re.sub(r',\\s*,', ',', clean_address)\n",
    "                clean_address = re.sub(r'\\s+', ' ', clean_address).strip()\n",
    "                postcode_pattern = r',?\\s*[A-Z]{1,2}\\d{1,2}[A-Z]?\\s*\\d[A-Z]{2}\\s*$'\n",
    "                clean_address = re.sub(postcode_pattern, '', clean_address, flags=re.IGNORECASE)\n",
    "                clean_address = clean_address.rstrip(', ')\n",
    "            else:\n",
    "                clean_address = 'N/A'\n",
    "            \n",
    "            outcode = address_data.get('outcode', '')\n",
    "            incode = address_data.get('incode', '')\n",
    "            \n",
    "            if outcode and incode:\n",
    "                postcode = f\"{outcode} {incode}\"\n",
    "            else:\n",
    "                postcode = 'N/A'\n",
    "            \n",
    "            return clean_address, postcode\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    def extract_tenure_details(self, json_data):\n",
    "        \"\"\"Extract tenure type and lease years remaining from JSON data\"\"\"\n",
    "        tenure_type = 'N/A'\n",
    "        lease_years_remaining = 'N/A'\n",
    "        \n",
    "        if json_data:\n",
    "            tenure_info = jmespath.search(\"tenure\", json_data)\n",
    "            \n",
    "            if isinstance(tenure_info, dict):\n",
    "                tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                years_remaining = tenure_info.get('yearsRemainingOnLease')\n",
    "                if years_remaining is not None:\n",
    "                    lease_years_remaining = years_remaining\n",
    "                \n",
    "                message = tenure_info.get('message')\n",
    "                if message:\n",
    "                    tenure_type = f\"{tenure_type} - {message}\"\n",
    "            elif isinstance(tenure_info, str):\n",
    "                tenure_type = tenure_info\n",
    "        \n",
    "        return tenure_type, lease_years_remaining\n",
    "\n",
    "    async def extract_property_json_data(self, property_url):\n",
    "        try:\n",
    "            response = await self.httpx_client.get(property_url)\n",
    "            data = Selector(response.text).xpath(\"//script[contains(.,'PAGE_MODEL = ')]/text()\").get()\n",
    "            if data:\n",
    "                json_objects = list(self.find_json_objects(data))\n",
    "                for obj in json_objects:\n",
    "                    if \"propertyData\" in obj:\n",
    "                        return obj[\"propertyData\"]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"JSON extraction error for {property_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def scrape_property_details_fast(self, property_url):\n",
    "        # Skip if already scraped\n",
    "        if property_url in self.scraped_urls:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            html_task = asyncio.create_task(self.httpx_client.get(property_url))\n",
    "            json_data = await self.extract_property_json_data(property_url)\n",
    "            response = await html_task\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            property_data = {'url': property_url}\n",
    "            \n",
    "            # Mark as scraped\n",
    "            self.scraped_urls.add(property_url)\n",
    "            \n",
    "            # Extract property ID and use as title\n",
    "            property_id = self.extract_property_id(property_url)\n",
    "            property_data['title'] = property_id\n",
    "            \n",
    "            # Basic property info with enhanced extraction\n",
    "            all_text = soup.get_text()\n",
    "            \n",
    "            # Enhanced bedroom extraction\n",
    "            bed_patterns = [\n",
    "                r'(\\d+)\\s*bedroom',\n",
    "                r'(\\d+)\\s*bed(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bed'\n",
    "            ]\n",
    "            property_data['bedrooms'] = 'N/A'\n",
    "            for pattern in bed_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bedrooms'] = match.group(1)\n",
    "                    break\n",
    "            \n",
    "            # Enhanced bathroom extraction\n",
    "            bath_patterns = [\n",
    "                r'(\\d+)\\s*bathroom',\n",
    "                r'(\\d+)\\s*bath(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bath'\n",
    "            ]\n",
    "            property_data['bathrooms'] = 'N/A'\n",
    "            for pattern in bath_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bathrooms'] = match.group(1)\n",
    "                    break\n",
    "\n",
    "            # Key features extraction\n",
    "            property_data['key_features'] = 'N/A'\n",
    "            feature_selectors = [\n",
    "                'ul.lIhZ24u1NHlVy5_W9V__6 li',\n",
    "                '.key-features li',\n",
    "                '[data-test=\"key-features\"] li'\n",
    "            ]\n",
    "            \n",
    "            for selector in feature_selectors:\n",
    "                features = soup.select(selector)\n",
    "                if features:\n",
    "                    feature_texts = [f.get_text().strip() for f in features]\n",
    "                    property_data['key_features'] = '; '.join(feature_texts)\n",
    "                    break\n",
    "            \n",
    "            if property_data['key_features'] == 'N/A':\n",
    "                feature_headers = soup.find_all(['h2', 'h3'], string=re.compile(r'key features|features|amenities', re.I))\n",
    "                for header in feature_headers:\n",
    "                    next_element = header.find_next_sibling()\n",
    "                    while next_element:\n",
    "                        if next_element.name in ['ul', 'ol']:\n",
    "                            features = [li.get_text().strip() for li in next_element.find_all('li')]\n",
    "                            if features:\n",
    "                                property_data['key_features'] = '; '.join(features)\n",
    "                                break\n",
    "                        next_element = next_element.find_next_sibling()\n",
    "                    if property_data['key_features'] != 'N/A':\n",
    "                        break\n",
    "            \n",
    "            # Property details extraction\n",
    "            property_data.update({\n",
    "                'parking': 'N/A',\n",
    "                'garden': 'N/A',\n",
    "                'council_tax': 'N/A',\n",
    "                'accessibility': 'N/A',\n",
    "                'size_sqft': 'N/A',\n",
    "                'size_sqm': 'N/A',\n",
    "                'furnish_status': 'N/A',\n",
    "            })\n",
    "           \n",
    "            # Extract details from various selectors\n",
    "            detail_selectors = [\n",
    "                'dt._17A0LehXZKxGHbPeiLQ1BI',\n",
    "                'dt[class*=\"detail\"]',\n",
    "                '.property-details dt',\n",
    "                'dl dt'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                detail_sections = soup.select(selector)\n",
    "                if detail_sections:\n",
    "                    for section in detail_sections:\n",
    "                        section_text = section.get_text().strip().upper()\n",
    "                        value_element = section.find_next_sibling(['dd', 'span', 'div'])\n",
    "                        value_text = value_element.get_text().strip() if value_element else 'N/A'\n",
    "                        \n",
    "                        if 'PARKING' in section_text:\n",
    "                            property_data['parking'] = value_text\n",
    "                        elif 'GARDEN' in section_text:\n",
    "                            property_data['garden'] = value_text\n",
    "                        elif 'COUNCIL TAX' in section_text:\n",
    "                            property_data['council_tax'] = value_text\n",
    "                        elif 'ACCESSIBILITY' in section_text:\n",
    "                            property_data['accessibility'] = value_text\n",
    "                        elif 'FURNISH' in section_text:\n",
    "                            property_data['furnish_status'] = value_text\n",
    "                    break\n",
    "\n",
    "            # Size extraction\n",
    "            sqft_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*ft',\n",
    "                r'([0-9,]+)\\s*sqft',\n",
    "                r'([0-9,]+)\\s*square\\s*feet'\n",
    "            ]\n",
    "            for pattern in sqft_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqft'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "            \n",
    "            sqm_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*m',\n",
    "                r'([0-9,]+)\\s*sqm',\n",
    "                r'([0-9,]+)\\s*square\\s*metres?'\n",
    "            ]\n",
    "            for pattern in sqm_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqm'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "\n",
    "            # Extract data from JSON if available\n",
    "            if json_data:\n",
    "                try:\n",
    "                    stations = jmespath.search(\"nearestStations[*].{name: name, distance: distance}\", json_data) or []\n",
    "                    photos = jmespath.search(\"images[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    floorplans = jmespath.search(\"floorplans[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    property_type = (jmespath.search(\"propertySubType\", json_data) or \n",
    "                                   jmespath.search(\"propertyType\", json_data) or 'N/A')\n",
    "                    description = jmespath.search(\"text.description\", json_data) or 'N/A'\n",
    "                    address_data = jmespath.search(\"address\", json_data)\n",
    "                    latitude = jmespath.search(\"location.latitude\", json_data) or 'N/A'\n",
    "                    longitude = jmespath.search(\"location.longitude\", json_data) or 'N/A'\n",
    "                    price = jmespath.search(\"prices.primaryPrice\", json_data) or 'N/A'\n",
    "                    \n",
    "                    tenure_type, lease_years_remaining = self.extract_tenure_details(json_data)\n",
    "                    formatted_address, postcode = self.format_address_and_postcode(address_data)\n",
    "                    \n",
    "                    property_data.update({\n",
    "                        'nearest_stations': '; '.join([s['name'] for s in stations]) or 'N/A',\n",
    "                        'station_distances': '; '.join([f\"{s['distance']} miles\" for s in stations]) or 'N/A',\n",
    "                        'station_count': len(stations),\n",
    "                        'image_count': len(photos),\n",
    "                        'image_urls': '; '.join([p['url'] for p in photos]) or 'N/A',\n",
    "                        'floorplan_count': len(floorplans),\n",
    "                        'floorplan_urls': '; '.join([f['url'] for f in floorplans]) or 'N/A',\n",
    "                        'property_type': property_type,\n",
    "                        'tenure_type': tenure_type,\n",
    "                        'lease_years_remaining': lease_years_remaining,\n",
    "                        'description': description,\n",
    "                        'latitude': latitude,\n",
    "                        'longitude': longitude,\n",
    "                        'address': formatted_address,\n",
    "                        'postcode': postcode,\n",
    "                        'price': price\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting JSON data: {e}\")\n",
    "            else:\n",
    "                # HTML fallback\n",
    "                property_type_fallback = 'N/A'\n",
    "                type_element = soup.find(string=re.compile(r'(flat|house|apartment|studio|maisonette)', re.I))\n",
    "                if type_element:\n",
    "                    property_type_fallback = type_element.strip()\n",
    "                \n",
    "                property_data.update({\n",
    "                    'nearest_stations': 'N/A',\n",
    "                    'station_distances': 'N/A', \n",
    "                    'station_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'image_urls': 'N/A',\n",
    "                    'floorplan_count': 0,\n",
    "                    'floorplan_urls': 'N/A',\n",
    "                    'property_type': property_type_fallback,\n",
    "                    'description': 'N/A',\n",
    "                    'tenure_type': 'N/A',\n",
    "                    'lease_years_remaining': 'N/A',\n",
    "                    'latitude': 'N/A',\n",
    "                    'longitude': 'N/A',\n",
    "                    'address': 'N/A',\n",
    "                    'postcode': 'N/A',\n",
    "                    'price': 'N/A'\n",
    "                })\n",
    "            \n",
    "            return property_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {property_url}: {e}\")\n",
    "            return {'url': property_url, 'error': str(e)}\n",
    "\n",
    "    async def scrape_search_directly(self, search_url, max_properties=None):\n",
    "    \"\"\"Directly scrape properties from search pages - stop when we hit repetitions\"\"\"\n",
    "    print(f\"Starting direct scrape of search URL...\")\n",
    "    \n",
    "    page = 0\n",
    "    consecutive_empty_pages = 0\n",
    "    consecutive_duplicate_pages = 0\n",
    "    new_properties_found = 0\n",
    "    \n",
    "    while True:\n",
    "        page_url = f\"{search_url}&index={page * 24}\"\n",
    "        \n",
    "        try:\n",
    "            # Get the search page\n",
    "            response = self.session.get(page_url, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find property links on this page\n",
    "            links = []\n",
    "            selectors = [\n",
    "                'a.propertyCard-link',\n",
    "                'a[href*=\"/properties/\"]',\n",
    "                '.propertyCard a',\n",
    "                '[data-test=\"property-card\"] a'\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                found_links = soup.select(selector)\n",
    "                if found_links:\n",
    "                    links = [f\"https://www.rightmove.co.uk{link.get('href')}\" \n",
    "                            for link in found_links if link.get('href') and '/properties/' in link.get('href')]\n",
    "                    break\n",
    "            \n",
    "            if len(links) == 0:\n",
    "                consecutive_empty_pages += 1\n",
    "                print(f\"Page {page + 1}: 0 properties found\")\n",
    "                # Increase threshold - stop after 5 consecutive empty pages instead of 2\n",
    "                if consecutive_empty_pages >= 5:\n",
    "                    print(f\"No more results found after {consecutive_empty_pages} empty pages. Stopping.\")\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_empty_pages = 0\n",
    "                \n",
    "                # Check how many are new vs duplicates\n",
    "                new_links = [link for link in links if link not in self.scraped_urls]\n",
    "                duplicate_count = len(links) - len(new_links)\n",
    "                \n",
    "                print(f\"Page {page + 1}: {len(links)} properties found, {len(new_links)} new, {duplicate_count} duplicates\")\n",
    "                \n",
    "                # If mostly duplicates, we might be hitting repeated content\n",
    "                if len(new_links) == 0:\n",
    "                    consecutive_duplicate_pages += 1\n",
    "                    # Increase threshold - stop after 5 consecutive pages with only duplicates\n",
    "                    if consecutive_duplicate_pages >= 5:\n",
    "                        print(f\"Too many consecutive pages with only duplicates. Stopping.\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_duplicate_pages = 0\n",
    "                \n",
    "                # Scrape the new properties directly\n",
    "                if new_links:\n",
    "                    batch_size = 3\n",
    "                    for i in range(0, len(new_links), batch_size):\n",
    "                        batch = new_links[i:i+batch_size]\n",
    "                        tasks = [self.scrape_property_details_fast(url) for url in batch]\n",
    "                        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "                        \n",
    "                        for result in results:\n",
    "                            if isinstance(result, dict) and result is not None and 'error' not in result:\n",
    "                                self.properties.append(result)\n",
    "                                new_properties_found += 1\n",
    "                        \n",
    "                        # Check if we've reached our target\n",
    "                        if max_properties and len(self.properties) >= max_properties:\n",
    "                            print(f\"Reached target of {max_properties} properties!\")\n",
    "                            return new_properties_found\n",
    "                        \n",
    "                        await asyncio.sleep(0.3)  # Small delay between batches\n",
    "            \n",
    "            page += 1  # THIS IS HOW WE MOVE TO NEXT PAGE\n",
    "            time.sleep(0.5)  # Delay between pages\n",
    "            \n",
    "            # Safety break to avoid infinite loops - increase limit\n",
    "            if page > 500:  # Increased from 200 to 500\n",
    "                print(f\"Reached maximum page limit (500 pages). Stopping.\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page + 1}: {e}\")\n",
    "            consecutive_empty_pages += 1\n",
    "            if consecutive_empty_pages >= 5:  # Increased threshold\n",
    "                break\n",
    "            page += 1  # Still move to next page even after error\n",
    "    \n",
    "    return new_properties_found\n",
    "\n",
    "    def save_to_csv(self, filename='rightmove_london_properties.csv'):\n",
    "        if not self.properties:\n",
    "            print(\"No properties to save!\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(self.properties)\n",
    "        \n",
    "        # Clean description field\n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].astype(str).apply(\n",
    "                lambda x: re.sub(r'<[^>]+>', ' ', x) if x != 'N/A' else x\n",
    "            ).apply(\n",
    "                lambda x: re.sub(r'\\s+', ' ', x).strip() if x != 'N/A' else x\n",
    "            )\n",
    "        \n",
    "        # Remove duplicate properties by URL (just in case)\n",
    "        df_clean = df.drop_duplicates(subset=['url'], keep='first')\n",
    "        \n",
    "        df_clean.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df_clean)} unique properties to {filename}\")\n",
    "        \n",
    "        if len(df_clean) > 0:\n",
    "            print(f\"\\nData Summary:\")\n",
    "            print(f\"Properties with prices: {len(df_clean[df_clean['price'] != 'N/A'])}\")\n",
    "            print(f\"Properties with addresses: {len(df_clean[df_clean['address'] != 'N/A'])}\")\n",
    "            print(f\"Properties with images: {len(df_clean[df_clean['image_count'] != 0])}\")\n",
    "        \n",
    "        return df_clean\n",
    "\n",
    "async def run_complete_scraper():\n",
    "    scraper = RightmoveRentalScraper()\n",
    "    \n",
    "    # London Boroughs\n",
    "    london_boroughs = [\n",
    "        {\"name\": \"City of London\", \"id\": \"REGION%5E61224\"},\n",
    "        {\"name\": \"Camden\", \"id\": \"REGION%5E93941\"},\n",
    "        {\"name\": \"Greenwich\", \"id\": \"REGION%5E61226\"},\n",
    "        {\"name\": \"Hackney\", \"id\": \"REGION%5E93953\"},\n",
    "        {\"name\": \"Hammersmith and Fulham\", \"id\": \"REGION%5E61407\"},\n",
    "        {\"name\": \"Islington\", \"id\": \"REGION%5E93965\"},\n",
    "        {\"name\": \"Kensington and Chelsea\", \"id\": \"REGION%5E61229\"},\n",
    "        {\"name\": \"Lambeth\", \"id\": \"REGION%5E93971\"},\n",
    "        {\"name\": \"Lewisham\", \"id\": \"REGION%5E61413\"},\n",
    "        {\"name\": \"Southwark\", \"id\": \"REGION%5E61518\"},\n",
    "        {\"name\": \"Tower Hamlets\", \"id\": \"REGION%5E61417\"},\n",
    "        {\"name\": \"Wandsworth\", \"id\": \"REGION%5E93977\"},\n",
    "        {\"name\": \"Westminster\", \"id\": \"REGION%5E93980\"},\n",
    "        {\"name\": \"Barking and Dagenham\", \"id\": \"REGION%5E61400\"},\n",
    "        {\"name\": \"Barnet\", \"id\": \"REGION%5E93929\"},\n",
    "        {\"name\": \"Bexley\", \"id\": \"REGION%5E93932\"},\n",
    "        {\"name\": \"Brent\", \"id\": \"REGION%5E93935\"},\n",
    "        {\"name\": \"Bromley\", \"id\": \"REGION%5E93938\"},\n",
    "        {\"name\": \"Croydon\", \"id\": \"REGION%5E93944\"},\n",
    "        {\"name\": \"Ealing\", \"id\": \"REGION%5E93947\"},\n",
    "        {\"name\": \"Enfield\", \"id\": \"REGION%5E93950\"},\n",
    "        {\"name\": \"Haringey\", \"id\": \"REGION%5E61227\"},\n",
    "        {\"name\": \"Harrow\", \"id\": \"REGION%5E93956\"},\n",
    "        {\"name\": \"Havering\", \"id\": \"REGION%5E61228\"},\n",
    "        {\"name\": \"Hillingdon\", \"id\": \"REGION%5E93959\"},\n",
    "        {\"name\": \"Hounslow\", \"id\": \"REGION%5E93962\"},\n",
    "        {\"name\": \"Kingston upon Thames\", \"id\": \"REGION%5E93968\"},\n",
    "        {\"name\": \"Merton\", \"id\": \"REGION%5E61414\"},\n",
    "        {\"name\": \"Newham\", \"id\": \"REGION%5E61231\"},\n",
    "        {\"name\": \"Redbridge\", \"id\": \"REGION%5E61537\"},\n",
    "        {\"name\": \"Richmond upon Thames\", \"id\": \"REGION%5E61415\"},\n",
    "        {\"name\": \"Sutton\", \"id\": \"REGION%5E93974\"},\n",
    "        {\"name\": \"Waltham Forest\", \"id\": \"REGION%5E61232\"}\n",
    "    ]\n",
    "    \n",
    "    # Price ranges\n",
    "    price_ranges = [\n",
    "        \"&maxPrice=400000\",\n",
    "        \"&minPrice=400000&maxPrice=600000\",\n",
    "        \"&minPrice=600000&maxPrice=800000\",\n",
    "        \"&minPrice=800000&maxPrice=1000000\",\n",
    "        \"&minPrice=1000000&maxPrice=1500000\",\n",
    "        \"&minPrice=1500000&maxPrice=2000000\",\n",
    "        \"&minPrice=2000000&maxPrice=3000000\",\n",
    "        \"&minPrice=3000000\"\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    target_properties = 50000\n",
    "    \n",
    "    for borough in london_boroughs:\n",
    "        for price_range in price_ranges:\n",
    "            search_url = f\"https://www.rightmove.co.uk/property-for-sale/find.html?useLocationIdentifier=true&locationIdentifier={borough['id']}&buy=For+sale&radius=0.0&_includeSSTC=on&index=0&sortType=2&channel=BUY&transactionType=BUY{price_range}\"\n",
    "            \n",
    "            print(f\"\\n🏙️ Scraping {borough['name']} - {price_range.replace('&', '')}\")\n",
    "            print(f\"Current total: {len(scraper.properties)} properties\")\n",
    "            \n",
    "            remaining_needed = target_properties - len(scraper.properties)\n",
    "            if remaining_needed <= 0:\n",
    "                print(f\"✅ Target of {target_properties} properties reached!\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                new_found = await scraper.scrape_search_directly(\n",
    "                    search_url, \n",
    "                    max_properties=remaining_needed\n",
    "                )\n",
    "                print(f\"✅ Found {new_found} new properties. Total: {len(scraper.properties)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {borough['name']}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if len(scraper.properties) >= target_properties:\n",
    "                print(f\"🎯 Target reached! Total: {len(scraper.properties)} properties\")\n",
    "                break\n",
    "            \n",
    "            await asyncio.sleep(1)\n",
    "        \n",
    "        if len(scraper.properties) >= target_properties:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    scraper.save_to_csv('rightmove_london_properties.csv')\n",
    "    await scraper.httpx_client.aclose()\n",
    "    \n",
    "    print(f\"⚡ Completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"📊 Final count: {len(scraper.properties)} unique London properties\")\n",
    "    return scraper.properties\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    results = asyncio.run(run_complete_scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4141ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏙️ Scraping City of London - maxPrice=400000\n",
      "Current total: 0 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 15 properties found, 15 new, 0 duplicates\n",
      "Page 2: 15 properties found, 0 new, 15 duplicates\n",
      "Page 3: 15 properties found, 0 new, 15 duplicates\n",
      "Page 4: 15 properties found, 0 new, 15 duplicates\n",
      "Page 5: 15 properties found, 0 new, 15 duplicates\n",
      "Page 6: 15 properties found, 0 new, 15 duplicates\n",
      "Page 7: 15 properties found, 0 new, 15 duplicates\n",
      "Page 8: 15 properties found, 0 new, 15 duplicates\n",
      "Page 9: 15 properties found, 0 new, 15 duplicates\n",
      "Page 10: 15 properties found, 0 new, 15 duplicates\n",
      "Page 11: 15 properties found, 0 new, 15 duplicates\n",
      "Too many consecutive pages with only duplicates. Stopping.\n",
      "✅ Found 14 new properties. Total: 14\n",
      "\n",
      "🏙️ Scraping City of London - minPrice=400000maxPrice=600000\n",
      "Current total: 14 properties\n",
      "Starting direct scrape of search URL...\n",
      "Page 1: 25 properties found, 25 new, 0 duplicates\n",
      "Page 2: 25 properties found, 1 new, 24 duplicates\n",
      "Page 3: 25 properties found, 0 new, 25 duplicates\n",
      "Page 4: 25 properties found, 0 new, 25 duplicates\n",
      "Page 5: 25 properties found, 0 new, 25 duplicates\n",
      "Page 6: 25 properties found, 0 new, 25 duplicates\n",
      "Page 7: 25 properties found, 0 new, 25 duplicates\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 541\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    540\u001b[0m     nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m--> 541\u001b[0m     results \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(run_complete_scraper())\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\events.py:88\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:396\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:303\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(self, exc)\u001b[0m\n\u001b[0;32m    301\u001b[0m _enter_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step_run_and_handle_result(exc)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     _leave_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[1], line 511\u001b[0m, in \u001b[0;36mrun_complete_scraper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m     new_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mscrape_search_directly(\n\u001b[0;32m    512\u001b[0m         search_url, \n\u001b[0;32m    513\u001b[0m         max_properties\u001b[38;5;241m=\u001b[39mremaining_needed\n\u001b[0;32m    514\u001b[0m     )\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new properties. Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(scraper\u001b[38;5;241m.\u001b[39mproperties)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[1], line 332\u001b[0m, in \u001b[0;36mRightmoveRentalScraper.scrape_search_directly\u001b[1;34m(self, search_url, max_properties)\u001b[0m\n\u001b[0;32m    328\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m24\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;66;03m# Get the search page\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(page_url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m    333\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# Find property links on this page\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Jc\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import jmespath\n",
    "import asyncio\n",
    "from httpx import AsyncClient\n",
    "from parsel import Selector\n",
    "import nest_asyncio\n",
    "\n",
    "class RightmoveRentalScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.properties = []\n",
    "        self.scraped_urls = set()\n",
    "        self.httpx_client = AsyncClient(\n",
    "            headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            },\n",
    "            follow_redirects=True,\n",
    "            http2=False,\n",
    "            timeout=30,\n",
    "        )\n",
    "    \n",
    "    def find_json_objects(self, text: str):\n",
    "        pos = 0\n",
    "        while True:\n",
    "            match = text.find(\"{\", pos)\n",
    "            if match == -1: break\n",
    "            try:\n",
    "                result, index = json.JSONDecoder().raw_decode(text[match:])\n",
    "                yield result\n",
    "                pos = match + index\n",
    "            except:\n",
    "                pos = match + 1\n",
    "\n",
    "    def extract_property_id(self, url):\n",
    "        \"\"\"Extract property ID from URL\"\"\"\n",
    "        match = re.search(r'/properties/(\\d+)', url)\n",
    "        return match.group(1) if match else 'N/A'\n",
    "    \n",
    "    def format_address_and_postcode(self, address_data):\n",
    "        \"\"\"Extract and format address and postcode from JSON address object\"\"\"\n",
    "        if isinstance(address_data, dict):\n",
    "            display_address = address_data.get('displayAddress', '')\n",
    "            if display_address:\n",
    "                clean_address = re.sub(r'\\r\\n|\\r|\\n', ', ', display_address)\n",
    "                clean_address = re.sub(r',\\s*,', ',', clean_address)\n",
    "                clean_address = re.sub(r'\\s+', ' ', clean_address).strip()\n",
    "                postcode_pattern = r',?\\s*[A-Z]{1,2}\\d{1,2}[A-Z]?\\s*\\d[A-Z]{2}\\s*$'\n",
    "                clean_address = re.sub(postcode_pattern, '', clean_address, flags=re.IGNORECASE)\n",
    "                clean_address = clean_address.rstrip(', ')\n",
    "            else:\n",
    "                clean_address = 'N/A'\n",
    "            \n",
    "            outcode = address_data.get('outcode', '')\n",
    "            incode = address_data.get('incode', '')\n",
    "            \n",
    "            if outcode and incode:\n",
    "                postcode = f\"{outcode} {incode}\"\n",
    "            else:\n",
    "                postcode = 'N/A'\n",
    "            \n",
    "            return clean_address, postcode\n",
    "        return 'N/A', 'N/A'\n",
    "    \n",
    "    def extract_tenure_details(self, json_data):\n",
    "        \"\"\"Extract tenure type and lease years remaining from JSON data\"\"\"\n",
    "        tenure_type = 'N/A'\n",
    "        lease_years_remaining = 'N/A'\n",
    "        \n",
    "        if json_data:\n",
    "            tenure_info = jmespath.search(\"tenure\", json_data)\n",
    "            \n",
    "            if isinstance(tenure_info, dict):\n",
    "                tenure_type = tenure_info.get('tenureType', 'N/A')\n",
    "                years_remaining = tenure_info.get('yearsRemainingOnLease')\n",
    "                if years_remaining is not None:\n",
    "                    lease_years_remaining = years_remaining\n",
    "                \n",
    "                message = tenure_info.get('message')\n",
    "                if message:\n",
    "                    tenure_type = f\"{tenure_type} - {message}\"\n",
    "            elif isinstance(tenure_info, str):\n",
    "                tenure_type = tenure_info\n",
    "        \n",
    "        return tenure_type, lease_years_remaining\n",
    "\n",
    "    async def extract_property_json_data(self, property_url):\n",
    "        try:\n",
    "            response = await self.httpx_client.get(property_url)\n",
    "            data = Selector(response.text).xpath(\"//script[contains(.,'PAGE_MODEL = ')]/text()\").get()\n",
    "            if data:\n",
    "                json_objects = list(self.find_json_objects(data))\n",
    "                for obj in json_objects:\n",
    "                    if \"propertyData\" in obj:\n",
    "                        return obj[\"propertyData\"]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"JSON extraction error for {property_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def scrape_property_details_fast(self, property_url):\n",
    "        # Skip if already scraped\n",
    "        if property_url in self.scraped_urls:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            html_task = asyncio.create_task(self.httpx_client.get(property_url))\n",
    "            json_data = await self.extract_property_json_data(property_url)\n",
    "            response = await html_task\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            property_data = {'url': property_url}\n",
    "            \n",
    "            # Mark as scraped\n",
    "            self.scraped_urls.add(property_url)\n",
    "            \n",
    "            # Extract property ID and use as title\n",
    "            property_id = self.extract_property_id(property_url)\n",
    "            property_data['title'] = property_id\n",
    "            \n",
    "            # Basic property info with enhanced extraction\n",
    "            all_text = soup.get_text()\n",
    "            \n",
    "            # Enhanced bedroom extraction\n",
    "            bed_patterns = [\n",
    "                r'(\\d+)\\s*bedroom',\n",
    "                r'(\\d+)\\s*bed(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bed'\n",
    "            ]\n",
    "            property_data['bedrooms'] = 'N/A'\n",
    "            for pattern in bed_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bedrooms'] = match.group(1)\n",
    "                    break\n",
    "            \n",
    "            # Enhanced bathroom extraction\n",
    "            bath_patterns = [\n",
    "                r'(\\d+)\\s*bathroom',\n",
    "                r'(\\d+)\\s*bath(?:room)?s?',\n",
    "                r'(\\d+)\\s*-?\\s*bath'\n",
    "            ]\n",
    "            property_data['bathrooms'] = 'N/A'\n",
    "            for pattern in bath_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['bathrooms'] = match.group(1)\n",
    "                    break\n",
    "\n",
    "            # Key features extraction\n",
    "            property_data['key_features'] = 'N/A'\n",
    "            feature_selectors = [\n",
    "                'ul.lIhZ24u1NHlVy5_W9V__6 li',\n",
    "                '.key-features li',\n",
    "                '[data-test=\"key-features\"] li'\n",
    "            ]\n",
    "            \n",
    "            for selector in feature_selectors:\n",
    "                features = soup.select(selector)\n",
    "                if features:\n",
    "                    feature_texts = [f.get_text().strip() for f in features]\n",
    "                    property_data['key_features'] = '; '.join(feature_texts)\n",
    "                    break\n",
    "            \n",
    "            if property_data['key_features'] == 'N/A':\n",
    "                feature_headers = soup.find_all(['h2', 'h3'], string=re.compile(r'key features|features|amenities', re.I))\n",
    "                for header in feature_headers:\n",
    "                    next_element = header.find_next_sibling()\n",
    "                    while next_element:\n",
    "                        if next_element.name in ['ul', 'ol']:\n",
    "                            features = [li.get_text().strip() for li in next_element.find_all('li')]\n",
    "                            if features:\n",
    "                                property_data['key_features'] = '; '.join(features)\n",
    "                                break\n",
    "                        next_element = next_element.find_next_sibling()\n",
    "                    if property_data['key_features'] != 'N/A':\n",
    "                        break\n",
    "            \n",
    "            # Property details extraction\n",
    "            property_data.update({\n",
    "                'parking': 'N/A',\n",
    "                'garden': 'N/A',\n",
    "                'council_tax': 'N/A',\n",
    "                'accessibility': 'N/A',\n",
    "                'size_sqft': 'N/A',\n",
    "                'size_sqm': 'N/A',\n",
    "                'furnish_status': 'N/A',\n",
    "            })\n",
    "           \n",
    "            # Extract details from various selectors\n",
    "            detail_selectors = [\n",
    "                'dt._17A0LehXZKxGHbPeiLQ1BI',\n",
    "                'dt[class*=\"detail\"]',\n",
    "                '.property-details dt',\n",
    "                'dl dt'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                detail_sections = soup.select(selector)\n",
    "                if detail_sections:\n",
    "                    for section in detail_sections:\n",
    "                        section_text = section.get_text().strip().upper()\n",
    "                        value_element = section.find_next_sibling(['dd', 'span', 'div'])\n",
    "                        value_text = value_element.get_text().strip() if value_element else 'N/A'\n",
    "                        \n",
    "                        if 'PARKING' in section_text:\n",
    "                            property_data['parking'] = value_text\n",
    "                        elif 'GARDEN' in section_text:\n",
    "                            property_data['garden'] = value_text\n",
    "                        elif 'COUNCIL TAX' in section_text:\n",
    "                            property_data['council_tax'] = value_text\n",
    "                        elif 'ACCESSIBILITY' in section_text:\n",
    "                            property_data['accessibility'] = value_text\n",
    "                        elif 'FURNISH' in section_text:\n",
    "                            property_data['furnish_status'] = value_text\n",
    "                    break\n",
    "\n",
    "            # Size extraction\n",
    "            sqft_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*ft',\n",
    "                r'([0-9,]+)\\s*sqft',\n",
    "                r'([0-9,]+)\\s*square\\s*feet'\n",
    "            ]\n",
    "            for pattern in sqft_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqft'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "            \n",
    "            sqm_patterns = [\n",
    "                r'([0-9,]+)\\s*sq\\.?\\s*m',\n",
    "                r'([0-9,]+)\\s*sqm',\n",
    "                r'([0-9,]+)\\s*square\\s*metres?'\n",
    "            ]\n",
    "            for pattern in sqm_patterns:\n",
    "                match = re.search(pattern, all_text, re.I)\n",
    "                if match:\n",
    "                    property_data['size_sqm'] = match.group(1).replace(',', '')\n",
    "                    break\n",
    "\n",
    "            # Extract data from JSON if available\n",
    "            if json_data:\n",
    "                try:\n",
    "                    stations = jmespath.search(\"nearestStations[*].{name: name, distance: distance}\", json_data) or []\n",
    "                    photos = jmespath.search(\"images[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    floorplans = jmespath.search(\"floorplans[*].{url: url, caption: caption}\", json_data) or []\n",
    "                    property_type = (jmespath.search(\"propertySubType\", json_data) or \n",
    "                                   jmespath.search(\"propertyType\", json_data) or 'N/A')\n",
    "                    description = jmespath.search(\"text.description\", json_data) or 'N/A'\n",
    "                    address_data = jmespath.search(\"address\", json_data)\n",
    "                    latitude = jmespath.search(\"location.latitude\", json_data) or 'N/A'\n",
    "                    longitude = jmespath.search(\"location.longitude\", json_data) or 'N/A'\n",
    "                    price = jmespath.search(\"prices.primaryPrice\", json_data) or 'N/A'\n",
    "                    \n",
    "                    tenure_type, lease_years_remaining = self.extract_tenure_details(json_data)\n",
    "                    formatted_address, postcode = self.format_address_and_postcode(address_data)\n",
    "                    \n",
    "                    property_data.update({\n",
    "                        'nearest_stations': '; '.join([s['name'] for s in stations]) or 'N/A',\n",
    "                        'station_distances': '; '.join([f\"{s['distance']} miles\" for s in stations]) or 'N/A',\n",
    "                        'station_count': len(stations),\n",
    "                        'image_count': len(photos),\n",
    "                        'image_urls': '; '.join([p['url'] for p in photos]) or 'N/A',\n",
    "                        'floorplan_count': len(floorplans),\n",
    "                        'floorplan_urls': '; '.join([f['url'] for f in floorplans]) or 'N/A',\n",
    "                        'property_type': property_type,\n",
    "                        'tenure_type': tenure_type,\n",
    "                        'lease_years_remaining': lease_years_remaining,\n",
    "                        'description': description,\n",
    "                        'latitude': latitude,\n",
    "                        'longitude': longitude,\n",
    "                        'address': formatted_address,\n",
    "                        'postcode': postcode,\n",
    "                        'price': price\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting JSON data: {e}\")\n",
    "            else:\n",
    "                # HTML fallback\n",
    "                property_type_fallback = 'N/A'\n",
    "                type_element = soup.find(string=re.compile(r'(flat|house|apartment|studio|maisonette)', re.I))\n",
    "                if type_element:\n",
    "                    property_type_fallback = type_element.strip()\n",
    "                \n",
    "                property_data.update({\n",
    "                    'nearest_stations': 'N/A',\n",
    "                    'station_distances': 'N/A', \n",
    "                    'station_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'image_urls': 'N/A',\n",
    "                    'floorplan_count': 0,\n",
    "                    'floorplan_urls': 'N/A',\n",
    "                    'property_type': property_type_fallback,\n",
    "                    'description': 'N/A',\n",
    "                    'tenure_type': 'N/A',\n",
    "                    'lease_years_remaining': 'N/A',\n",
    "                    'latitude': 'N/A',\n",
    "                    'longitude': 'N/A',\n",
    "                    'address': 'N/A',\n",
    "                    'postcode': 'N/A',\n",
    "                    'price': 'N/A'\n",
    "                })\n",
    "            \n",
    "            return property_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {property_url}: {e}\")\n",
    "            return {'url': property_url, 'error': str(e)}\n",
    "\n",
    "    async def scrape_search_directly(self, search_url, max_properties=None):\n",
    "        \"\"\"Directly scrape properties from search pages - continue until no more results\"\"\"\n",
    "        print(f\"Starting direct scrape of search URL...\")\n",
    "        \n",
    "        page = 0  # ✅ Fixed variable name\n",
    "        consecutive_empty_pages = 0\n",
    "        consecutive_duplicate_pages = 0\n",
    "        new_properties_found = 0\n",
    "        \n",
    "        while True:\n",
    "            page_url = f\"{search_url}&index={page * 24}\"\n",
    "            \n",
    "            try:\n",
    "                # Get the search page\n",
    "                response = self.session.get(page_url, timeout=15)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find property links on this page\n",
    "                links = []\n",
    "                selectors = [\n",
    "                    'a.propertyCard-link',\n",
    "                    'a[href*=\"/properties/\"]',\n",
    "                    '.propertyCard a',\n",
    "                    '[data-test=\"property-card\"] a'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors:\n",
    "                    found_links = soup.select(selector)\n",
    "                    if found_links:\n",
    "                        links = [f\"https://www.rightmove.co.uk{link.get('href')}\" \n",
    "                                for link in found_links if link.get('href') and '/properties/' in link.get('href')]\n",
    "                        break\n",
    "                \n",
    "                if len(links) == 0:\n",
    "                    consecutive_empty_pages += 1\n",
    "                    print(f\"Page {page + 1}: 0 properties found\")\n",
    "                    # ✅ Increased threshold - stop after 10 consecutive empty pages\n",
    "                    if consecutive_empty_pages >= 10:\n",
    "                        print(f\"No more results found after {consecutive_empty_pages} empty pages. Stopping.\")\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_empty_pages = 0\n",
    "                    \n",
    "                    # Check how many are new vs duplicates\n",
    "                    new_links = [link for link in links if link not in self.scraped_urls]\n",
    "                    duplicate_count = len(links) - len(new_links)\n",
    "                    \n",
    "                    print(f\"Page {page + 1}: {len(links)} properties found, {len(new_links)} new, {duplicate_count} duplicates\")\n",
    "                    \n",
    "                    # If mostly duplicates, we might be hitting repeated content\n",
    "                    if len(new_links) == 0:\n",
    "                        consecutive_duplicate_pages += 1\n",
    "                        # ✅ Increased threshold - stop after 10 consecutive pages with only duplicates\n",
    "                        if consecutive_duplicate_pages >= 10:\n",
    "                            print(f\"Too many consecutive pages with only duplicates. Stopping.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        consecutive_duplicate_pages = 0\n",
    "                    \n",
    "                    # Scrape the new properties directly\n",
    "                    if new_links:\n",
    "                        batch_size = 3\n",
    "                        for i in range(0, len(new_links), batch_size):\n",
    "                            batch = new_links[i:i+batch_size]\n",
    "                            tasks = [self.scrape_property_details_fast(url) for url in batch]\n",
    "                            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "                            \n",
    "                            for result in results:\n",
    "                                if isinstance(result, dict) and result is not None and 'error' not in result:\n",
    "                                    self.properties.append(result)\n",
    "                                    new_properties_found += 1\n",
    "                            \n",
    "                            # Check if we've reached our target\n",
    "                            if max_properties and len(self.properties) >= max_properties:\n",
    "                                print(f\"Reached target of {max_properties} properties!\")\n",
    "                                return new_properties_found\n",
    "                            \n",
    "                            await asyncio.sleep(0.3)  # Small delay between batches\n",
    "                \n",
    "                page += 1  # ✅ Move to next page\n",
    "                time.sleep(0.5)  # Delay between pages\n",
    "                \n",
    "                # Safety break to avoid infinite loops\n",
    "                if page > 1000:  # ✅ Increased limit\n",
    "                    print(f\"Reached maximum page limit (1000 pages). Stopping.\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page + 1}: {e}\")\n",
    "                consecutive_empty_pages += 1\n",
    "                if consecutive_empty_pages >= 10:  # ✅ Increased threshold\n",
    "                    break\n",
    "                page += 1  # Still move to next page even after error\n",
    "        \n",
    "        return new_properties_found\n",
    "\n",
    "    def save_to_csv(self, filename='rightmove_london_properties.csv'):\n",
    "        if not self.properties:\n",
    "            print(\"No properties to save!\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.DataFrame(self.properties)\n",
    "        \n",
    "        # Clean description field\n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].astype(str).apply(\n",
    "                lambda x: re.sub(r'<[^>]+>', ' ', x) if x != 'N/A' else x\n",
    "            ).apply(\n",
    "                lambda x: re.sub(r'\\s+', ' ', x).strip() if x != 'N/A' else x\n",
    "            )\n",
    "        \n",
    "        # Remove duplicate properties by URL\n",
    "        df_clean = df.drop_duplicates(subset=['url'], keep='first')\n",
    "        \n",
    "        df_clean.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df_clean)} unique properties to {filename}\")\n",
    "        \n",
    "        if len(df_clean) > 0:\n",
    "            print(f\"\\nData Summary:\")\n",
    "            print(f\"Properties with prices: {len(df_clean[df_clean['price'] != 'N/A'])}\")\n",
    "            print(f\"Properties with addresses: {len(df_clean[df_clean['address'] != 'N/A'])}\")\n",
    "            print(f\"Properties with images: {len(df_clean[df_clean['image_count'] != 0])}\")\n",
    "        \n",
    "        return df_clean\n",
    "\n",
    "async def run_complete_scraper():\n",
    "    scraper = RightmoveRentalScraper()\n",
    "    \n",
    "    # London Boroughs - ✅ Using correct location identifiers\n",
    "    london_boroughs = [\n",
    "        {\"name\": \"City of London\", \"id\": \"REGION%5E61224\"},\n",
    "        {\"name\": \"Camden\", \"id\": \"REGION%5E93941\"},\n",
    "        {\"name\": \"Greenwich\", \"id\": \"REGION%5E61226\"},\n",
    "        {\"name\": \"Hackney\", \"id\": \"REGION%5E93953\"},\n",
    "        {\"name\": \"Hammersmith and Fulham\", \"id\": \"REGION%5E61407\"},\n",
    "        {\"name\": \"Islington\", \"id\": \"REGION%5E93965\"},\n",
    "        {\"name\": \"Kensington and Chelsea\", \"id\": \"REGION%5E61229\"},\n",
    "        {\"name\": \"Lambeth\", \"id\": \"REGION%5E93971\"},\n",
    "        {\"name\": \"Lewisham\", \"id\": \"REGION%5E61413\"},\n",
    "        {\"name\": \"Southwark\", \"id\": \"REGION%5E61518\"},\n",
    "        {\"name\": \"Tower Hamlets\", \"id\": \"REGION%5E61417\"},\n",
    "        {\"name\": \"Wandsworth\", \"id\": \"REGION%5E93977\"},\n",
    "        {\"name\": \"Westminster\", \"id\": \"REGION%5E93980\"},\n",
    "        {\"name\": \"Barking and Dagenham\", \"id\": \"REGION%5E61400\"},\n",
    "        {\"name\": \"Barnet\", \"id\": \"REGION%5E93929\"},\n",
    "        {\"name\": \"Bexley\", \"id\": \"REGION%5E93932\"},\n",
    "        {\"name\": \"Brent\", \"id\": \"REGION%5E93935\"},\n",
    "        {\"name\": \"Bromley\", \"id\": \"REGION%5E93938\"},\n",
    "        {\"name\": \"Croydon\", \"id\": \"REGION%5E93944\"},\n",
    "        {\"name\": \"Ealing\", \"id\": \"REGION%5E93947\"},\n",
    "        {\"name\": \"Enfield\", \"id\": \"REGION%5E93950\"},\n",
    "        {\"name\": \"Haringey\", \"id\": \"REGION%5E61227\"},\n",
    "        {\"name\": \"Harrow\", \"id\": \"REGION%5E93956\"},\n",
    "        {\"name\": \"Havering\", \"id\": \"REGION%5E61228\"},\n",
    "        {\"name\": \"Hillingdon\", \"id\": \"REGION%5E93959\"},\n",
    "        {\"name\": \"Hounslow\", \"id\": \"REGION%5E93962\"},\n",
    "        {\"name\": \"Kingston upon Thames\", \"id\": \"REGION%5E93968\"},\n",
    "        {\"name\": \"Merton\", \"id\": \"REGION%5E61414\"},\n",
    "        {\"name\": \"Newham\", \"id\": \"REGION%5E61231\"},\n",
    "        {\"name\": \"Redbridge\", \"id\": \"REGION%5E61537\"},\n",
    "        {\"name\": \"Richmond upon Thames\", \"id\": \"REGION%5E61415\"},\n",
    "        {\"name\": \"Sutton\", \"id\": \"REGION%5E93974\"},\n",
    "        {\"name\": \"Waltham Forest\", \"id\": \"REGION%5E61232\"}\n",
    "    ]\n",
    "    \n",
    "    # Price ranges\n",
    "    price_ranges = [\n",
    "        \"&maxPrice=400000\",\n",
    "        \"&minPrice=400000&maxPrice=600000\",\n",
    "        \"&minPrice=600000&maxPrice=800000\",\n",
    "        \"&minPrice=800000&maxPrice=1000000\",\n",
    "        \"&minPrice=1000000&maxPrice=1500000\",\n",
    "        \"&minPrice=1500000&maxPrice=2000000\",\n",
    "        \"&minPrice=2000000&maxPrice=3000000\",\n",
    "        \"&minPrice=3000000\"\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    target_properties = 50000\n",
    "    \n",
    "    for borough in london_boroughs:\n",
    "        for price_range in price_ranges:\n",
    "            search_url = f\"https://www.rightmove.co.uk/property-for-sale/find.html?useLocationIdentifier=true&locationIdentifier={borough['id']}&buy=For+sale&radius=0.0&_includeSSTC=on&index=0&sortType=2&channel=BUY&transactionType=BUY{price_range}\"\n",
    "            \n",
    "            print(f\"\\n🏙️ Scraping {borough['name']} - {price_range.replace('&', '')}\")\n",
    "            print(f\"Current total: {len(scraper.properties)} properties\")\n",
    "            \n",
    "            remaining_needed = target_properties - len(scraper.properties)\n",
    "            if remaining_needed <= 0:\n",
    "                print(f\"✅ Target of {target_properties} properties reached!\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                new_found = await scraper.scrape_search_directly(\n",
    "                    search_url, \n",
    "                    max_properties=remaining_needed\n",
    "                )\n",
    "                print(f\"✅ Found {new_found} new properties. Total: {len(scraper.properties)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {borough['name']}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if len(scraper.properties) >= target_properties:\n",
    "                print(f\"🎯 Target reached! Total: {len(scraper.properties)} properties\")\n",
    "                break\n",
    "            \n",
    "            await asyncio.sleep(1)\n",
    "        \n",
    "        if len(scraper.properties) >= target_properties:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    scraper.save_to_csv('rightmove_london_properties.csv')\n",
    "    await scraper.httpx_client.aclose()\n",
    "    \n",
    "    print(f\"⚡ Completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"📊 Final count: {len(scraper.properties)} unique London properties\")\n",
    "    return scraper.properties\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    results = asyncio.run(run_complete_scraper())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
